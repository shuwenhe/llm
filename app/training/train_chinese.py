"""ç®€åŒ–ç‰ˆä¸­æ–‡æ–‡æœ¬è®­ç»ƒ - ä½¿ç”¨ç¤ºä¾‹æ•°æ®"""
import os
import torch
import numpy as np
from pathlib import Path
from tqdm import tqdm
from torch.utils.data import DataLoader

from app.modeling.config import ModelConfig
from app.modeling.model import GPT
from app.modeling.data import TextDataset, load_tokenizer


def create_sample_chinese_corpus():
    """åˆ›å»ºç¤ºä¾‹ä¸­æ–‡è¯­æ–™åº“ç”¨äºæ¼”ç¤º"""
    texts = [
        "åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½ï¼Œä½äºååŒ—å¹³åŸçš„ä¸­éƒ¨ã€‚",
        "é•¿åŸæ˜¯ä¸­å›½å¤ä»£æœ€ä¼Ÿå¤§çš„å»ºç­‘ä¹‹ä¸€ã€‚",
        "ä¸­å›½æœ‰ç€æ‚ ä¹…çš„å†å²æ–‡åŒ–ä¼ ç»Ÿã€‚",
        "æŠ€æœ¯åˆ›æ–°æ¨åŠ¨äº†ç¤¾ä¼šçš„å‘å±•å’Œè¿›æ­¥ã€‚",
        "è¯­è¨€æ˜¯äººç±»æœ€é‡è¦çš„äº¤æµå·¥å…·ã€‚",
        "æ•™è‚²æ˜¯å‘å±•å›½å®¶çš„åŸºç¡€å’Œå…³é”®ã€‚",
        "ç§‘å­¦ç ”ç©¶å¯¹äººç±»æ–‡æ˜è¿›æ­¥è‡³å…³é‡è¦ã€‚",
        "æ–‡å­¦ä½œå“åæ˜ äº†ä¸åŒæ—¶ä»£çš„ç¤¾ä¼šç°å®ã€‚",
        "éŸ³ä¹è‰ºæœ¯æ˜¯äººç±»æ–‡åŒ–é—äº§çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚",
        "ä½“è‚²è¿åŠ¨ä¿ƒè¿›äº†èº«å¿ƒçš„å¥åº·å‘å±•ã€‚",
    ] * 100  # é‡å¤ä»¥å¢åŠ æ•°æ®é‡
    
    return texts


def train_chinese_text(
    learning_rate=1e-4,
    batch_size=4,
    num_epochs=1,
    checkpoint_path="checkpoints/model.pt",
    output_path="checkpoints/model.pt",
    data_file=None,
    save_every_epoch=True,
    keep_last_n=3,
):
    """è®­ç»ƒä¸­æ–‡æ–‡æœ¬èƒ½åŠ›
    
    Args:
        data_file: ä¸­æ–‡æ–‡æœ¬æ•°æ®æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨ç¤ºä¾‹æ•°æ®
        save_every_epoch: æ˜¯å¦ä¿å­˜æ¯ä¸ªepochçš„æ£€æŸ¥ç‚¹
        keep_last_n: ä¿ç•™æœ€è¿‘Nä¸ªæ£€æŸ¥ç‚¹ï¼ˆ0è¡¨ç¤ºä¿ç•™æ‰€æœ‰ï¼‰
    """
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸš€ è®¾å¤‡: {device}")
    
    # åˆ›å»ºcheckpointsç›®å½•
    checkpoint_dir = Path(output_path).parent
    checkpoint_dir.mkdir(exist_ok=True)
    
    # åŠ è½½æˆ–åˆ›å»ºæ¨¡å‹
    print("ğŸ“‹ åŠ è½½æ¨¡å‹...")
    start_epoch = 0
    if os.path.exists(checkpoint_path):
        checkpoint = torch.load(checkpoint_path, map_location="cpu")
        model_config = ModelConfig(**checkpoint["model_config"])
        model = GPT(model_config)
        model.load_state_dict(checkpoint["model"])
        start_epoch = checkpoint.get('epoch', 0) + 1
        print(f"âœ“ ä» {checkpoint_path} æ¢å¤è®­ç»ƒ (ä» Epoch {start_epoch} å¼€å§‹)")
    else:
        model_config = ModelConfig()
        model = GPT(model_config)
        print(f"âœ“ åˆ›å»ºæ–°æ¨¡å‹")
    
    model = model.to(device)
    
    # åŠ è½½æ•°æ®
    if data_file and os.path.exists(data_file):
        print(f"\nğŸ“š ä»æ–‡ä»¶åŠ è½½æ•°æ®: {data_file}")
        with open(data_file, "r", encoding="utf-8") as f:
            texts = [line.strip() for line in f if line.strip()]
        print(f"âœ“ åŠ è½½äº† {len(texts)} æ¡æ–‡æœ¬")
    else:
        if data_file:
            print(f"\nâš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        print(f"ğŸ“š ä½¿ç”¨ç¤ºä¾‹ä¸­æ–‡æ•°æ®é›†...")
        texts = create_sample_chinese_corpus()
        print(f"âœ“ åˆ›å»ºäº† {len(texts)} æ¡ç¤ºä¾‹æ–‡æœ¬")
    
    # Tokenize
    tokenizer = load_tokenizer()
    print(f"ğŸ”¤ Tokenizing æ•°æ®...")
    all_tokens = []
    for text in tqdm(texts, desc="Tokenizing"):
        tokens = tokenizer.encode(text)
        all_tokens.extend(tokens)
    
    all_tokens = np.array(all_tokens, dtype=np.uint32)
    print(f"âœ“ æ€»å…± {len(all_tokens):,} ä¸ª tokens")
    
    # åˆ›å»ºæ•°æ®é›†
    print(f"\nğŸ“¦ åˆ›å»ºæ•°æ®é›† (block_size={model_config.block_size})...")
    train_size = int(len(all_tokens) * 0.8)
    
    train_tokens = all_tokens[:train_size]
    val_tokens = all_tokens[train_size:]
    
    train_dataset = TextDataset(train_tokens, model_config.block_size)
    val_dataset = TextDataset(val_tokens, model_config.block_size)
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size)
    
    print(f"âœ“ è®­ç»ƒé›†: {len(train_dataset):,} æ ·æœ¬")
    print(f"âœ“ éªŒè¯é›†: {len(val_dataset):,} æ ·æœ¬")
    
    # è®­ç»ƒé…ç½®
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)
    
    print(f"\n{'='*70}")
    print(f"ğŸ“ è®­ç»ƒé…ç½®")
    print(f"{'='*70}")
    print(f"  è®¾å¤‡: {device}")
    print(f"  å­¦ä¹ ç‡: {learning_rate}")
    print(f"  æ‰¹å¤§å°: {batch_size}")
    print(f"  è®­ç»ƒè½®æ•°: {num_epochs}")
    print(f"  è®­ç»ƒæ ·æœ¬: {len(train_dataset):,} ä¸ª")
    print(f"  éªŒè¯æ ·æœ¬: {len(val_dataset):,} ä¸ª")
    print(f"  æ¯è½®æ­¥æ•°: {len(train_loader):,} æ­¥")
    print(f"  æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")
    print(f"  ä¿å­˜è·¯å¾„: {output_path}")
    print(f"{'='*70}\n")
    
    best_loss = float('inf')
    
    import time
    training_start = time.time()
    
    # è®­ç»ƒå†å²è®°å½•
    history = {
        'train_loss': [],
        'val_loss': [],
        'epochs': []
    }
    
    for epoch in range(start_epoch, start_epoch + num_epochs):
        epoch_start = time.time()
        print(f"\n{'='*70}")
        print(f"ğŸ“š Epoch {epoch+1}/{num_epochs}")
        print(f"{'='*70}")
        
        model.train()
        epoch_loss = 0
        epoch_batches = 0
        
        # è®­ç»ƒè¿›åº¦æ¡æ·»åŠ å®æ—¶lossæ˜¾ç¤º
        progress_bar = tqdm(train_loader, desc=f"è®­ç»ƒä¸­")
        for x, y in progress_bar:
            x = x.to(device)
            y = y.to(device)
            
            # å‰å‘ä¼ æ’­
            logits, loss = model(x, y)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            epoch_loss += loss.item()
            epoch_batches += 1
            
            # å®æ—¶æ˜¾ç¤ºå½“å‰loss
            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'avg_loss': f'{epoch_loss/epoch_batches:.4f}'
            })
        
        # è®¡ç®—æŸå¤±
        avg_epoch_loss = epoch_loss / max(1, epoch_batches)
        
        # éªŒè¯
        model.eval()
        val_loss = 0
        val_batches = 0
        
        print(f"\nğŸ” éªŒè¯ä¸­...")
        with torch.no_grad():
            for x, y in tqdm(val_loader, desc="éªŒè¯"):
                x = x.to(device)
                y = y.to(device)
                logits, loss = model(x, y)
                val_loss += loss.item()
                val_batches += 1
        
        avg_val_loss = val_loss / max(1, val_batches)
        epoch_time = time.time() - epoch_start
        
        # è®°å½•å†å²
        history['train_loss'].append(avg_epoch_loss)
        history['val_loss'].append(avg_val_loss)
        history['epochs'].append(epoch + 1)
        
        # æ˜¾ç¤ºç»“æœ
        print(f"\n{'='*70}")
        print(f"ğŸ“Š Epoch {epoch+1}/{start_epoch + num_epochs} ç»“æœ")
        print(f"{'='*70}")
        print(f"  â±ï¸  ç”¨æ—¶: {epoch_time:.1f}s ({epoch_time/60:.1f}min)")
        print(f"  ğŸ“‰ è®­ç»ƒæŸå¤±: {avg_epoch_loss:.4f}")
        print(f"  ğŸ“Š éªŒè¯æŸå¤±: {avg_val_loss:.4f}")
        
        # ä¿å­˜æ£€æŸ¥ç‚¹çš„é€šç”¨å‡½æ•°
        def save_checkpoint(path, is_best=False):
            torch.save({
                'model': model.state_dict(),
                'model_config': model_config.__dict__,
                'optimizer': optimizer.state_dict(),
                'epoch': epoch,
                'best_loss': best_loss,
                'train_loss': avg_epoch_loss,
                'val_loss': avg_val_loss,
                'history': history,
            }, path)
        
        print(f"\nğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹...")
        
        # 1. ä¿å­˜æ¯ä¸ªepochçš„æ£€æŸ¥ç‚¹
        if save_every_epoch:
            epoch_checkpoint = checkpoint_dir / f"model_epoch_{epoch+1}.pt"
            save_checkpoint(epoch_checkpoint)
            size_mb = epoch_checkpoint.stat().st_size / (1024*1024)
            print(f"  âœ“ Epochæ£€æŸ¥ç‚¹: {epoch_checkpoint.name} ({size_mb:.1f}MB)")
            
            # æ¸…ç†æ—§æ£€æŸ¥ç‚¹ï¼ˆä¿ç•™æœ€è¿‘Nä¸ªï¼‰
            if keep_last_n > 0:
                epoch_files = sorted(checkpoint_dir.glob("model_epoch_*.pt"))
                if len(epoch_files) > keep_last_n:
                    for old_file in epoch_files[:-keep_last_n]:
                        old_file.unlink()
                        print(f"  ğŸ—‘ï¸  åˆ é™¤æ—§æ£€æŸ¥ç‚¹: {old_file.name}")
        
        # 2. ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_val_loss < best_loss:
            improvement = best_loss - avg_val_loss
            best_loss = avg_val_loss
            
            best_model_path = checkpoint_dir / "best_model.pt"
            save_checkpoint(best_model_path, is_best=True)
            size_mb = best_model_path.stat().st_size / (1024*1024)
            print(f"  ğŸ† æœ€ä½³æ¨¡å‹: {best_model_path.name} ({size_mb:.1f}MB) [æ”¹è¿›: {improvement:.4f}]")
        else:
            print(f"  â„¹ï¸  éªŒè¯æŸå¤±æœªæ”¹è¿› (æœ€ä½³: {best_loss:.4f})")
        
        # 3. å§‹ç»ˆä¿å­˜æœ€æ–°æ¨¡å‹ï¼ˆç”¨äºæ–­ç‚¹ç»­è®­ï¼‰
        latest_path = checkpoint_dir / "latest.pt"
        save_checkpoint(latest_path)
        size_mb = latest_path.stat().st_size / (1024*1024)
        print(f"  ğŸ“Œ æœ€æ–°æ¨¡å‹: {latest_path.name} ({size_mb:.1f}MB) [ç”¨äºæ¢å¤è®­ç»ƒ]")
        
        # 4. æ›´æ–°ä¸»æ£€æŸ¥ç‚¹ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
        save_checkpoint(output_path)
        size_mb = Path(output_path).stat().st_size / (1024*1024)
        print(f"  ğŸ“ ä¸»æ£€æŸ¥ç‚¹: {Path(output_path).name} ({size_mb:.1f}MB)")
        
        print(f"{'='*70}")
    
    training_time = time.time() - training_start
    print(f"\n{'='*70}")
    print(f"âœ… è®­ç»ƒå®Œæˆ!")
    print(f"{'='*70}")
    print(f"  â±ï¸  æ€»ç”¨æ—¶: {training_time:.1f}s ({training_time/60:.1f}min)")
    print(f"  ğŸ“Š æœ€ä½³éªŒè¯æŸå¤±: {best_loss:.4f}")
    print(f"\nğŸ“ ä¿å­˜çš„æ¨¡å‹:")
    print(f"  ğŸ† æœ€ä½³æ¨¡å‹: checkpoints/best_model.pt")
    print(f"  ğŸ“Œ æœ€æ–°æ¨¡å‹: checkpoints/latest.pt")
    if save_every_epoch:
        print(f"  ğŸ“¦ Epochæ£€æŸ¥ç‚¹: checkpoints/model_epoch_*.pt (æœ€è¿‘{keep_last_n}ä¸ª)")
    print(f"\nğŸš€ ä½¿ç”¨è®­ç»ƒåçš„æ¨¡å‹:")
    print(f"  # ä½¿ç”¨æœ€ä½³æ¨¡å‹")
    print(f"  LLM_CHECKPOINT=checkpoints/best_model.pt make serve")
    print(f"  # æˆ–ç»§ç»­è®­ç»ƒ")
    print(f"  python train_chinese.py --checkpoint checkpoints/latest.pt --epochs 3")
    print(f"{'='*70}\n")
    
    # ä¿å­˜è®­ç»ƒå†å²
    history_file = checkpoint_dir / "training_history.json"
    import json
    with open(history_file, 'w') as f:
        json.dump(history, f, indent=2)
    print(f"ğŸ“ˆ è®­ç»ƒå†å²å·²ä¿å­˜: {history_file}")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ä¸­æ–‡æ–‡æœ¬è®­ç»ƒ")
    parser.add_argument("--learning-rate", type=float, default=1e-4)
    parser.add_argument("--batch-size", type=int, default=4)
    parser.add_argument("--epochs", type=int, default=1)
    parser.add_argument("--checkpoint", default="checkpoints/model.pt", help="åŠ è½½çš„æ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument("--output", default="checkpoints/model.pt", help="è¾“å‡ºæ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument("--data-file", help="ä¸­æ–‡æ–‡æœ¬æ•°æ®æ–‡ä»¶è·¯å¾„ (å¦‚: data/zh_wiki.txt)")
    parser.add_argument("--save-every-epoch", action="store_true", default=True, help="ä¿å­˜æ¯ä¸ªepochçš„æ£€æŸ¥ç‚¹")
    parser.add_argument("--keep-last-n", type=int, default=3, help="ä¿ç•™æœ€è¿‘Nä¸ªepochæ£€æŸ¥ç‚¹")
    parser.add_argument("--resume", action="store_true", help="ä»æœ€æ–°æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ")
    
    args = parser.parse_args()
    
    # å¦‚æœæŒ‡å®šresumeï¼Œä½¿ç”¨latest.pt
    checkpoint_path = args.checkpoint
    if args.resume:
        latest = Path(args.output).parent / "latest.pt"
        if latest.exists():
            checkpoint_path = str(latest)
            print(f"ğŸ”„ ä»æœ€æ–°æ£€æŸ¥ç‚¹æ¢å¤: {checkpoint_path}")
    
    train_chinese_text(
        learning_rate=args.learning_rate,
        batch_size=args.batch_size,
        num_epochs=args.epochs,
        checkpoint_path=checkpoint_path,
        output_path=args.output,
        data_file=args.data_file,
        save_every_epoch=args.save_every_epoch,
        keep_last_n=args.keep_last_n,
    )
