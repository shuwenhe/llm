"""自研后端训练主链路（纯 numpy）"""

import argparse
import os
import pickle

import numpy as np

from app.core.models import TransformerLM
from app.core.optim import AdamW
from app.core.tokenizer import CharTokenizer


def _sample_corpus():
    return [
        "北京是中国的首都，位于华北平原中部。",
        "人工智能正在改变世界，推动社会进步。",
        "语言模型可以生成文本，理解语义信息。",
        "机器学习需要数据和算力，还需要算法。",
        "模型训练要关注损失函数下降，学习率很重要。",
        "深度学习在计算机视觉领域取得了巨大成就。",
        "自然语言处理技术在电商和搜索引擎中广泛应用。",
        "神经网络通过反向传播算法实现参数更新。",
        "数据预处理对模型训练的质量有重要影响。",
        "特征工程是机器学习中的关键环节。",
        "超参数调优需要在验证集上反复测试。",
        "正则化技术可以防止模型过拟合。",
        "批归一化加快了神经网络的训练速度。",
        "注意力机制大幅提高了序列模型的性能。",
        "transformers架构革命了自然语言处理。",
        "迁移学习让我们可以利用预训练模型。",
        "多任务学习能够提高模型的泛化能力。",
        "知识蒸馏可以将大模型压缩为小模型。",
        "梯度裁剪防止了训练过程中的梯度爆炸。",
        "学习率预热有助于模型的稳定训练。",
        "对数据进行增强可以扩大训练集的规模。",
        "交叉验证是评估模型性能的重要方法。",
        "集成学习通过多个模型的组合提高准确率。",
        "强化学习让机器可以通过奖励学习策略。",
        "生成对抗网络可以生成逼真的虚假数据。",
    ] * 20


def _make_batches(token_ids, batch_size, seq_len):
    token_ids = np.asarray(token_ids, dtype=np.int64)
    max_start = len(token_ids) - seq_len - 1
    while True:
        starts = np.random.randint(0, max_start, size=batch_size)
        x = np.stack([token_ids[s:s + seq_len] for s in starts], axis=0)
        y = np.stack([token_ids[s + 1:s + seq_len + 1] for s in starts], axis=0)
        yield x, y


def train_core(batch_size=8, epochs=3, learning_rate=3e-3, output="checkpoints/model_core.pkl"):
    np.random.seed(42)

    texts = _sample_corpus()
    tokenizer = CharTokenizer.from_texts(texts)
    all_tokens = []
    for t in texts:
        all_tokens.extend(tokenizer.encode(t))

    seq_len = 64
    model = TransformerLM(
        vocab_size=tokenizer.vocab_size,
        n_embd=512,
        n_layers=4,
        n_heads=8,
        max_seq_len=seq_len,
        dropout=0.1
    )
    optimizer = AdamW(model.parameters(), lr=learning_rate)

    steps_per_epoch = 10
    total_steps = max(1, epochs * steps_per_epoch)
    batches = _make_batches(all_tokens, batch_size=batch_size, seq_len=seq_len)

    losses = []
    for step in range(total_steps):
        x, y = next(batches)
        optimizer.zero_grad()
        _, loss = model(x, y)
        loss.backward()
        optimizer.step()
        losses.append(loss.item())
        if step % 5 == 0 or step == total_steps - 1:
            print(f"step {step+1}/{total_steps}: loss={loss.item():.4f}")

    os.makedirs(os.path.dirname(output) or ".", exist_ok=True)
    state_dict = {f"param_{i}": p.data.copy() for i, p in enumerate(model.parameters())}
    payload = {
        "backend": "core",
        "model": {
            "vocab_size": tokenizer.vocab_size,
            "n_embd": 512,
            "n_layers": 4,
            "n_heads": 8,
            "max_seq_len": seq_len,
            "dropout": 0.1,
            "state_dict": state_dict,
        },
        "tokenizer": tokenizer.to_dict(),
        "metrics": {
            "start_loss": losses[0],
            "end_loss": losses[-1],
        },
    }
    with open(output, "wb") as f:
        pickle.dump(payload, f)

    print("✅ core 训练完成")
    print(f"   start_loss={losses[0]:.4f}, end_loss={losses[-1]:.4f}")
    print(f"   checkpoint={output}")


def main():
    parser = argparse.ArgumentParser(description="core backend training")
    parser.add_argument("--batch-size", type=int, default=8)
    parser.add_argument("--epochs", type=int, default=1)
    parser.add_argument("--learning-rate", type=float, default=3e-3)
    parser.add_argument("--output", type=str, default="checkpoints/model_core.pkl")
    parser.add_argument("--checkpoint", type=str, default="", help="保留参数兼容，当前未使用")
    args = parser.parse_args()

    train_core(
        batch_size=args.batch_size,
        epochs=args.epochs,
        learning_rate=args.learning_rate,
        output=args.output,
    )


if __name__ == "__main__":
    main()
