"""core å¿«é€Ÿæ–‡æœ¬ç”Ÿæˆæµ‹è¯•å®ç°"""

import os
import pickle

import numpy as np

from app.core.models import TinyLM, TransformerLM
from app.core.tokenizer import CharTokenizer


def quick_test():
    checkpoint_path = os.getenv("LLM_CHECKPOINT", "checkpoints/model_core.pkl")

    if not os.path.exists(checkpoint_path):
        print(f"âŒ æ¨¡å‹æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}\n")
        print("è¯·å…ˆè®­ç»ƒæ¨¡å‹:")
        print("  make train-core")
        print("  make train-chinese")
        return

    print("ğŸš€ å¿«é€Ÿç”Ÿæˆæµ‹è¯•")
    print("åç«¯: core")
    print("=" * 60)

    with open(checkpoint_path, "rb") as f:
        payload = pickle.load(f)
    tokenizer = CharTokenizer.from_dict(payload["tokenizer"])
    model_cfg = payload["model"]
    if all(k in model_cfg for k in ("n_layers", "n_heads", "max_seq_len")):
        model = TransformerLM(
            vocab_size=model_cfg["vocab_size"],
            n_embd=model_cfg["n_embd"],
            n_layers=model_cfg["n_layers"],
            n_heads=model_cfg["n_heads"],
            max_seq_len=model_cfg["max_seq_len"],
            dropout=model_cfg.get("dropout", 0.1),
        )
    else:
        model = TinyLM(vocab_size=model_cfg["vocab_size"], n_embd=model_cfg["n_embd"])
    state_dict = model_cfg["state_dict"]
    for i, p in enumerate(model.parameters()):
        key = f"param_{i}"
        if key not in state_dict:
            raise ValueError(f"checkpoint ç¼ºå°‘å‚æ•°: {key}")
        src = state_dict[key]
        if p.data.shape != src.shape:
            raise ValueError(
                f"checkpoint å‚æ•°å½¢çŠ¶ä¸åŒ¹é…: {key}, src={src.shape}, dst={p.data.shape}"
            )
        p.data[...] = src

    test_prompts = [
        "Once upon a time",
        "The meaning of life is",
        "In a world where",
    ]

    test_configs = [
        {"name": "ä¿å®ˆæ¨¡å¼", "temp": 0.7, "tokens": 80},
        {"name": "å¹³è¡¡æ¨¡å¼", "temp": 0.8, "tokens": 120},
        {"name": "åˆ›æ„æ¨¡å¼", "temp": 1.0, "tokens": 150},
    ]

    for prompt in test_prompts:
        print(f"\n{'='*60}")
        print(f"ğŸ“ æç¤ºè¯: \"{prompt}\"")
        print(f"{'='*60}")

        for cfg in test_configs:
            print(f"\nğŸ”§ {cfg['name']} (temp={cfg['temp']}, tokens={cfg['tokens']})")
            print("-" * 60)

            ids = tokenizer.encode(prompt)
            if not ids:
                ids = [0]

            max_ctx = getattr(model, "max_seq_len", None)
            for _ in range(cfg["tokens"]):
                ctx = ids[-max_ctx:] if isinstance(max_ctx, int) and max_ctx > 0 else ids
                x = np.array([ctx], dtype=np.int64)
                logits, _ = model(x, None)
                next_logits = logits.data[0, -1] / max(cfg["temp"], 1e-6)
                next_logits = next_logits - np.max(next_logits)
                probs = np.exp(next_logits)
                probs = probs / (probs.sum() + 1e-12)
                next_id = int(np.random.choice(len(probs), p=probs))
                ids.append(next_id)

            generated_text = tokenizer.decode(ids)
            print(generated_text)

    print("\n" + "=" * 60)
    print("âœ… æµ‹è¯•å®Œæˆï¼é€‰æ‹©æ•ˆæœæœ€å¥½çš„å‚æ•°åœ¨ generate.py ä¸­ä½¿ç”¨")


if __name__ == "__main__":
    quick_test()
