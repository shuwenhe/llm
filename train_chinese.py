"""ç®€åŒ–ç‰ˆä¸­æ–‡æ–‡æœ¬è®­ç»ƒ - ä½¿ç”¨ç¤ºä¾‹æ•°æ®"""
import os
import torch
import numpy as np
from pathlib import Path
from tqdm import tqdm
from torch.utils.data import DataLoader

from config import ModelConfig
from model import GPT
from data import TextDataset, load_tokenizer


def create_sample_chinese_corpus():
    """åˆ›å»ºç¤ºä¾‹ä¸­æ–‡è¯­æ–™åº“ç”¨äºæ¼”ç¤º"""
    texts = [
        "åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½ï¼Œä½äºååŒ—å¹³åŸçš„ä¸­éƒ¨ã€‚",
        "é•¿åŸæ˜¯ä¸­å›½å¤ä»£æœ€ä¼Ÿå¤§çš„å»ºç­‘ä¹‹ä¸€ã€‚",
        "ä¸­å›½æœ‰ç€æ‚ ä¹…çš„å†å²æ–‡åŒ–ä¼ ç»Ÿã€‚",
        "æŠ€æœ¯åˆ›æ–°æ¨åŠ¨äº†ç¤¾ä¼šçš„å‘å±•å’Œè¿›æ­¥ã€‚",
        "è¯­è¨€æ˜¯äººç±»æœ€é‡è¦çš„äº¤æµå·¥å…·ã€‚",
        "æ•™è‚²æ˜¯å‘å±•å›½å®¶çš„åŸºç¡€å’Œå…³é”®ã€‚",
        "ç§‘å­¦ç ”ç©¶å¯¹äººç±»æ–‡æ˜è¿›æ­¥è‡³å…³é‡è¦ã€‚",
        "æ–‡å­¦ä½œå“åæ˜ äº†ä¸åŒæ—¶ä»£çš„ç¤¾ä¼šç°å®ã€‚",
        "éŸ³ä¹è‰ºæœ¯æ˜¯äººç±»æ–‡åŒ–é—äº§çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚",
        "ä½“è‚²è¿åŠ¨ä¿ƒè¿›äº†èº«å¿ƒçš„å¥åº·å‘å±•ã€‚",
    ] * 100  # é‡å¤ä»¥å¢åŠ æ•°æ®é‡
    
    return texts


def train_chinese_text(
    learning_rate=1e-4,
    batch_size=4,
    num_epochs=1,
    checkpoint_path="checkpoints/model.pt",
    output_path="checkpoints/model.pt",
    data_file=None,
):
    """è®­ç»ƒä¸­æ–‡æ–‡æœ¬èƒ½åŠ›
    
    Args:
        data_file: ä¸­æ–‡æ–‡æœ¬æ•°æ®æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨ç¤ºä¾‹æ•°æ®
    """
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸš€ è®¾å¤‡: {device}")
    
    # åŠ è½½æˆ–åˆ›å»ºæ¨¡å‹
    print("ğŸ“‹ åŠ è½½æ¨¡å‹...")
    if os.path.exists(checkpoint_path):
        checkpoint = torch.load(checkpoint_path, map_location="cpu")
        model_config = ModelConfig(**checkpoint["model_config"])
        model = GPT(model_config)
        model.load_state_dict(checkpoint["model"])
        print(f"âœ“ ä» {checkpoint_path} åŠ è½½æ¨¡å‹")
    else:
        model_config = ModelConfig()
        model = GPT(model_config)
        print(f"âœ“ åˆ›å»ºæ–°æ¨¡å‹")
    
    model = model.to(device)
    
    # åŠ è½½æ•°æ®
    if data_file and os.path.exists(data_file):
        print(f"\nğŸ“š ä»æ–‡ä»¶åŠ è½½æ•°æ®: {data_file}")
        with open(data_file, "r", encoding="utf-8") as f:
            texts = [line.strip() for line in f if line.strip()]
        print(f"âœ“ åŠ è½½äº† {len(texts)} æ¡æ–‡æœ¬")
    else:
        if data_file:
            print(f"\nâš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        print(f"ğŸ“š ä½¿ç”¨ç¤ºä¾‹ä¸­æ–‡æ•°æ®é›†...")
        texts = create_sample_chinese_corpus()
        print(f"âœ“ åˆ›å»ºäº† {len(texts)} æ¡ç¤ºä¾‹æ–‡æœ¬")
    
    # Tokenize
    tokenizer = load_tokenizer()
    print(f"ğŸ”¤ Tokenizing æ•°æ®...")
    all_tokens = []
    for text in tqdm(texts, desc="Tokenizing"):
        tokens = tokenizer.encode(text)
        all_tokens.extend(tokens)
    
    all_tokens = np.array(all_tokens, dtype=np.uint32)
    print(f"âœ“ æ€»å…± {len(all_tokens):,} ä¸ª tokens")
    
    # åˆ›å»ºæ•°æ®é›†
    print(f"\nğŸ“¦ åˆ›å»ºæ•°æ®é›† (block_size={model_config.block_size})...")
    train_size = int(len(all_tokens) * 0.8)
    
    train_tokens = all_tokens[:train_size]
    val_tokens = all_tokens[train_size:]
    
    train_dataset = TextDataset(train_tokens, model_config.block_size)
    val_dataset = TextDataset(val_tokens, model_config.block_size)
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size)
    
    print(f"âœ“ è®­ç»ƒé›†: {len(train_dataset):,} æ ·æœ¬")
    print(f"âœ“ éªŒè¯é›†: {len(val_dataset):,} æ ·æœ¬")
    
    # è®­ç»ƒé…ç½®
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)
    
    print(f"\n{'='*70}")
    print(f"ğŸ“ è®­ç»ƒé…ç½®")
    print(f"{'='*70}")
    print(f"  è®¾å¤‡: {device}")
    print(f"  å­¦ä¹ ç‡: {learning_rate}")
    print(f"  æ‰¹å¤§å°: {batch_size}")
    print(f"  è®­ç»ƒè½®æ•°: {num_epochs}")
    print(f"  è®­ç»ƒæ ·æœ¬: {len(train_dataset):,} ä¸ª")
    print(f"  éªŒè¯æ ·æœ¬: {len(val_dataset):,} ä¸ª")
    print(f"  æ¯è½®æ­¥æ•°: {len(train_loader):,} æ­¥")
    print(f"  æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")
    print(f"  ä¿å­˜è·¯å¾„: {output_path}")
    print(f"{'='*70}\n")
    
    best_loss = float('inf')
    
    import time
    training_start = time.time()
    
    for epoch in range(num_epochs):
        epoch_start = time.time()
        print(f"\n{'='*70}")
        print(f"ğŸ“š Epoch {epoch+1}/{num_epochs}")
        print(f"{'='*70}")
        
        model.train()
        epoch_loss = 0
        epoch_batches = 0
        
        # è®­ç»ƒè¿›åº¦æ¡æ·»åŠ å®æ—¶lossæ˜¾ç¤º
        progress_bar = tqdm(train_loader, desc=f"è®­ç»ƒä¸­")
        for x, y in progress_bar:
            x = x.to(device)
            y = y.to(device)
            
            # å‰å‘ä¼ æ’­
            logits, loss = model(x, y)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            epoch_loss += loss.item()
            epoch_batches += 1
            
            # å®æ—¶æ˜¾ç¤ºå½“å‰loss
            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'avg_loss': f'{epoch_loss/epoch_batches:.4f}'
            })
        
        # è®¡ç®—æŸå¤±
        avg_epoch_loss = epoch_loss / max(1, epoch_batches)
        
        # éªŒè¯
        model.eval()
        val_loss = 0
        val_batches = 0
        
        print(f"\nğŸ” éªŒè¯ä¸­...")
        with torch.no_grad():
            for x, y in tqdm(val_loader, desc="éªŒè¯"):
                x = x.to(device)
                y = y.to(device)
                logits, loss = model(x, y)
                val_loss += loss.item()
                val_batches += 1
        
        avg_val_loss = val_loss / max(1, val_batches)
        epoch_time = time.time() - epoch_start
        
        # æ˜¾ç¤ºç»“æœ
        print(f"\n{'â”€'*70}")
        print(f"ğŸ“Š Epoch {epoch+1} ç»“æœ:")
        print(f"{'â”€'*70}")
        print(f"  â±ï¸  ç”¨æ—¶: {epoch_time:.1f}s ({epoch_time/60:.1f}min)")
        print(f"  ğŸ“‰ è®­ç»ƒæŸå¤±: {avg_epoch_loss:.4f}")
        print(f"  ğŸ“Š éªŒè¯æŸå¤±: {avg_val_loss:.4f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_val_loss < best_loss:
            improvement = best_loss - avg_val_loss
            best_loss = avg_val_loss
            print(f"  âœ¨ æ–°çš„æœ€ä½³æŸå¤±! (æå‡: {improvement:.4f})")
            print(f"  ğŸ’¾ ä¿å­˜æ¨¡å‹åˆ°: {output_path}")
            
            torch.save({
                'model': model.state_dict(),
                'model_config': model_config.__dict__,
                'optimizer': optimizer.state_dict(),
                'epoch': epoch,
                'best_loss': best_loss,
            }, output_path)
        else:
            print(f"  â„¹ï¸  æœªæ”¹è¿› (æœ€ä½³: {best_loss:.4f})")
        print(f"{'â”€'*70}")
    
    training_time = time.time() - training_start
    print(f"\n{'='*70}")
    print(f"âœ… è®­ç»ƒå®Œæˆ!")
    print(f"{'='*70}")
    print(f"  â±ï¸  æ€»ç”¨æ—¶: {training_time:.1f}s ({training_time/60:.1f}min)")
    print(f"  ğŸ“Š æœ€ä½³éªŒè¯æŸå¤±: {best_loss:.4f}")
    print(f"  ğŸ’¾ æ¨¡å‹ä¿å­˜äº: {output_path}")
    print(f"\nğŸš€ ä½¿ç”¨è®­ç»ƒåçš„æ¨¡å‹:")
    print(f"  make serve")
    print(f"  æˆ–è€…: python serve.py")
    print(f"{'='*70}\n")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ä¸­æ–‡æ–‡æœ¬è®­ç»ƒ")
    parser.add_argument("--learning-rate", type=float, default=1e-4)
    parser.add_argument("--batch-size", type=int, default=4)
    parser.add_argument("--epochs", type=int, default=1)
    parser.add_argument("--checkpoint", default="checkpoints/model.pt")
    parser.add_argument("--output", default="checkpoints/model.pt")
    parser.add_argument("--data-file", help="ä¸­æ–‡æ–‡æœ¬æ•°æ®æ–‡ä»¶è·¯å¾„ (å¦‚: data/zh_wiki.txt)")
    
    args = parser.parse_args()
    
    train_chinese_text(
        learning_rate=args.learning_rate,
        batch_size=args.batch_size,
        num_epochs=args.epochs,
        checkpoint_path=args.checkpoint,
        output_path=args.output,
        data_file=args.data_file,
    )
