# LLM é¡¹ç›®ä¸­çš„æ•°å­¦çŸ¥è¯†åˆ†æ

è¿™ä¸ª GPT é£æ ¼çš„å¤§è¯­è¨€æ¨¡å‹é¡¹ç›®æ¶‰åŠå¤šä¸ªæ•°å­¦é¢†åŸŸçš„çŸ¥è¯†ã€‚æœ¬æ–‡æ¡£è¯¦ç»†åˆ†æé¡¹ç›®ä¸­ç”¨åˆ°çš„æ•°å­¦åŸç†ã€‚

## ğŸ“‘ ç›®å½•

1. [çº¿æ€§ä»£æ•°](#çº¿æ€§ä»£æ•°)
2. [å¾®ç§¯åˆ†ä¸ä¼˜åŒ–](#å¾®ç§¯åˆ†ä¸ä¼˜åŒ–)
3. [æ¦‚ç‡è®ºä¸ç»Ÿè®¡](#æ¦‚ç‡è®ºä¸ç»Ÿè®¡)
4. [ä¿¡æ¯è®º](#ä¿¡æ¯è®º)
5. [æ•°å€¼åˆ†æ](#æ•°å€¼åˆ†æ)
6. [ä¸‰è§’å‡½æ•°](#ä¸‰è§’å‡½æ•°)
7. [å¤æ‚åº¦åˆ†æ](#å¤æ‚åº¦åˆ†æ)

---

## çº¿æ€§ä»£æ•°

### 1. çŸ©é˜µä¹˜æ³•ä¸å¼ é‡è¿ç®—

**ä½ç½®**: [model.py](../model.py) - `CausalSelfAttention` ç±»

**åº”ç”¨**:
- æŸ¥è¯¢ã€é”®ã€å€¼çš„çº¿æ€§å˜æ¢
- æ³¨æ„åŠ›è®¡ç®—

**æ•°å­¦**:
```
Q = XÂ·W_q  (B, T, d_model) @ (d_model, d_model) â†’ (B, T, d_model)
K = XÂ·W_k
V = XÂ·W_v
```

**ä»£ç **:
```python
# [model.py, ç¬¬ 27 è¡Œ]
self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)

# [model.py, ç¬¬ 43 è¡Œ] çº¿æ€§å˜æ¢
q, k, v = self.c_attn(x).split(self.n_embd, dim=2)
```

### 2. çŸ©é˜µä¹˜æ³•ï¼šæ³¨æ„åŠ›åˆ†æ•°

**æ•°å­¦**:
$$\text{Attention} = \text{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V$$

**ä»£ç **:
```python
# [model.py, ç¬¬ 51 è¡Œ]
att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
```

è¿™é‡Œï¼š
- `q @ k.transpose(-2, -1)` è®¡ç®— $Q K^T$
- `math.sqrt(k.size(-1))` æ˜¯ç¼©æ”¾å› å­ $\sqrt{d_k}$

### 3. å‘é‡å˜å½¢ä¸è½¬ç½®

**æ•°å­¦**: å¼ é‡é‡å¡‘æ“ä½œ
```
(B, T, C) â†’ (B, nh, T, hs)  å…¶ä¸­ C = nh Ã— hs
```

**ä»£ç **:
```python
# [model.py, ç¬¬ 45-48 è¡Œ]
k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
```

### 4. æƒé‡çŸ©é˜µåˆå§‹åŒ–

**åº”ç”¨**: ç¥ç»ç½‘ç»œæƒé‡åˆå§‹åŒ–å½±å“æ”¶æ•›é€Ÿåº¦

**ä»£ç **:
```python
# [model.py, ç¬¬ 169 è¡Œ] æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–
torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)

# [model.py, ç¬¬ 174 è¡Œ] æ®‹å·®è¿æ¥ç‰¹æ®Šç¼©æ”¾
torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))
```

è¿™ä½¿ç”¨äº†æ­£æ€åˆ†å¸ƒ $N(0, \sigma^2)$ï¼Œå…¶ä¸­ $\sigma = 0.02$

---

## å¾®ç§¯åˆ†ä¸ä¼˜åŒ–

### 1. é“¾å¼æ³•åˆ™ä¸åå‘ä¼ æ’­

**åŸç†**: ç¥ç»ç½‘ç»œä½¿ç”¨é“¾å¼æ³•åˆ™è®¡ç®—æ¢¯åº¦

**æ•°å­¦**:
$$\frac{\partial L}{\partial w} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial w}$$

**ä»£ç **:
```python
# [train.py, ç¬¬ 121 è¡Œ]
loss.backward()  # PyTorch è‡ªåŠ¨è®¡ç®—æ¢¯åº¦
```

### 2. æ¢¯åº¦ä¸‹é™ä¸ Adam ä¼˜åŒ–å™¨

**åº”ç”¨**: æ›´æ–°ç½‘ç»œå‚æ•°

**ä»£ç **:
```python
# [train.py, ç¬¬ 102-108 è¡Œ]
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=train_config.learning_rate,
    betas=(train_config.beta1, train_config.beta2),
    weight_decay=train_config.weight_decay
)
```

**Adam ç®—æ³•**çš„æ›´æ–°è§„åˆ™ï¼š
```
m_t = Î²â‚Â·m_{t-1} + (1-Î²â‚)Â·g_t         (ä¸€é˜¶åŠ¨é‡)
v_t = Î²â‚‚Â·v_{t-1} + (1-Î²â‚‚)Â·g_tÂ²       (äºŒé˜¶åŠ¨é‡)
w_t = w_{t-1} - Î±Â·m_t / (âˆšv_t + Îµ)
```

å…¶ä¸­ï¼š
- `beta1 = 0.9` ï¼ˆ[config.py](../config.py) ç¬¬ 35 è¡Œï¼‰
- `beta2 = 0.95` ï¼ˆ[config.py](../config.py) ç¬¬ 36 è¡Œï¼‰

### 3. å­¦ä¹ ç‡è°ƒåº¦ï¼šWarmup + Cosine Decay

**ä½ç½®**: [train.py](../train.py) - `get_lr()` å‡½æ•°

**åˆ†ä¸‰é˜¶æ®µ**:

#### é˜¶æ®µ 1: çº¿æ€§ Warmup
$$\alpha_t = \alpha_{\max} \cdot \frac{t}{t_{warmup}}$$

```python
# [train.py, ç¬¬ 15 è¡Œ]
if it < config.warmup_iters:
    return config.learning_rate * it / config.warmup_iters
```

#### é˜¶æ®µ 3: Cosine è¡°å‡
$$\alpha_t = \alpha_{\min} + \frac{1 + \cos(\pi \cdot \frac{t - t_{warmup}}{t_{max} - t_{warmup}})}{2} \cdot (\alpha_{\max} - \alpha_{\min})$$

```python
# [train.py, ç¬¬ 20-24 è¡Œ]
decay_ratio = (it - config.warmup_iters) / (config.lr_decay_iters - config.warmup_iters)
coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))
return config.min_lr + coeff * (config.learning_rate - config.min_lr)
```

### 4. æ¢¯åº¦è£å‰ª

**åŸç†**: é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸

**ä»£ç **:
```python
# [train.py, ç¬¬ 126-128 è¡Œ]
if train_config.grad_clip != 0.0:
    torch.nn.utils.clip_grad_norm_(model.parameters(), train_config.grad_clip)
```

è¿™é™åˆ¶æ¢¯åº¦çš„èŒƒæ•°ï¼š$\|\nabla\| \leq \text{grad\_clip}$

---

## æ¦‚ç‡è®ºä¸ç»Ÿè®¡

### 1. Softmax å‡½æ•°

**åº”ç”¨**: å°†æ³¨æ„åŠ›åˆ†æ•°è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ

**æ•°å­¦**:
$$\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}$$

**ä»£ç **:
```python
# [model.py, ç¬¬ 54 è¡Œ]
att = F.softmax(att, dim=-1)
```

**æ€§è´¨**:
- è¾“å‡ºå’Œä¸º 1
- æ‰€æœ‰è¾“å‡ºéè´Ÿ
- æ•°å€¼ç¨³å®šçš„å®ç°æ–¹æ³•ï¼š$\text{softmax}(x) = \text{softmax}(x - \max(x))$

### 2. äº¤å‰ç†µæŸå¤±

**ä½ç½®**: [model.py](../model.py) - `forward()` æ–¹æ³•

**æ•°å­¦**:
$$L = -\frac{1}{N}\sum_{i=1}^{N} y_i \log(\hat{y}_i)$$

å…¶ä¸­ $y_i$ æ˜¯çœŸå®æ ‡ç­¾ï¼Œ$\hat{y}_i$ æ˜¯æ¨¡å‹é¢„æµ‹çš„æ¦‚ç‡ã€‚

**ä»£ç **:
```python
# [model.py, ç¬¬ 149 è¡Œ]
loss = F.cross_entropy(logits.view(-1, logits.size(-1)), 
                       targets.view(-1), 
                       ignore_index=-1)
```

### 3. Dropout æ­£åˆ™åŒ–

**åŸç†**: éšæœºä¸¢å¼ƒç¥ç»å…ƒï¼Œå‡å°‘è¿‡æ‹Ÿåˆ

**æ•°å­¦**: åœ¨è®­ç»ƒæ—¶ï¼Œæ¯ä¸ªç¥ç»å…ƒä»¥æ¦‚ç‡ $p$ è¢«ä¿ç•™ï¼Œè¾“å‡ºä¹˜ä»¥ $\frac{1}{1-p}$ æ¥è¡¥å¿

**ä»£ç **:
```python
# [model.py, ç¬¬ 35-36 è¡Œ]
self.attn_dropout = nn.Dropout(config.dropout)
self.resid_dropout = nn.Dropout(config.dropout)
# å…¶ä¸­ config.dropout = 0.1
```

### 4. åŠ æƒå¹³å‡å’ŒæœŸæœ›

**åº”ç”¨**: è®¡ç®—éªŒè¯æŸå¤±çš„å¹³å‡å€¼

**ä»£ç **:
```python
# [train.py, ç¬¬ 42 è¡Œ]
return sum(losses) / len(losses)
```

è¿™è®¡ç®—æœŸæœ›å€¼ $E[L] = \frac{1}{n}\sum_{i=1}^{n} L_i$

---

## ä¿¡æ¯è®º

### 1. ä¿¡æ¯ç†µä¸äº¤å‰ç†µ

**å…³ç³»**:
$$H(P, Q) = H(P) + D_{KL}(P||Q)$$

å…¶ä¸­ï¼š
- $H(P, Q)$ æ˜¯äº¤å‰ç†µ
- $H(P)$ æ˜¯çœŸå®åˆ†å¸ƒçš„ç†µ
- $D_{KL}(P||Q)$ æ˜¯ KL æ•£åº¦

### 2. KL æ•£åº¦ï¼ˆç›¸å¯¹ç†µï¼‰

**å®šä¹‰**:
$$D_{KL}(P||Q) = \sum_i P(i) \log\frac{P(i)}{Q(i)}$$

äº¤å‰ç†µæŸå¤±æœ€å°åŒ–æ—¶ï¼Œç­‰ä»·äºæœ€å°åŒ– KL æ•£åº¦ã€‚

---

## æ•°å€¼åˆ†æ

### 1. æ•°å€¼ç¨³å®šæ€§ï¼šSoftmax

æ ‡å‡†è®¡ç®— $\text{softmax}(x) = \frac{e^{x}}{\sum e^{x}}$ å¯èƒ½å¯¼è‡´æº¢å‡ºã€‚

**ç¨³å®šæ–¹æ³•**:
$$\text{softmax}(x_i) = \frac{e^{x_i - \max(x)}}{\sum_j e^{x_j - \max(x)}}$$

PyTorch å†…éƒ¨è‡ªåŠ¨å¤„ç†è¿™ä¸€ç‚¹ã€‚

### 2. æ··åˆç²¾åº¦è®­ç»ƒ

**ä»£ç **:
```python
# [train.py, ç¬¬ 116-117 è¡Œ]
with torch.amp.autocast(device_type='cuda', dtype=torch.float16):
    logits, loss = model(x, y)
```

ä½¿ç”¨ float16 åŠ å¿«è®¡ç®—ï¼ŒåŒæ—¶ä¿æŒ float32 å¤„ç†æ•æ„Ÿæ“ä½œã€‚

### 3. Layer Normalization

**æ•°å­¦**:
$$y = \gamma \cdot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta$$

å…¶ä¸­ï¼š
- $\mu = \frac{1}{m}\sum_i x_i$ ï¼ˆå‡å€¼ï¼‰
- $\sigma^2 = \frac{1}{m}\sum_i (x_i - \mu)^2$ ï¼ˆæ–¹å·®ï¼‰
- $\epsilon = 1e^{-5}$ ï¼ˆé˜²æ­¢é™¤ä»¥é›¶ï¼‰

**ä»£ç **:
```python
# [model.py, ç¬¬ 15 è¡Œ]
return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)
```

### 4. GELU æ¿€æ´»å‡½æ•°

**å®šä¹‰**:
$$\text{GELU}(x) = x \cdot \Phi(x)$$

å…¶ä¸­ $\Phi(x)$ æ˜¯æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ã€‚

**è¿‘ä¼¼**:
$$\text{GELU}(x) \approx 0.5x(1 + \tanh(\sqrt{\frac{2}{\pi}}(x + 0.044715x^3)))$$

**ä»£ç **:
```python
# [model.py, ç¬¬ 74 è¡Œ]
self.gelu = nn.GELU()
```

---

## ä¸‰è§’å‡½æ•°

### 1. ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰

**æ•°å­¦**:
$$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right)$$
$$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right)$$

**ç›®çš„**: ä¸ºä¸åŒä½ç½®çš„ tokens æä¾›ä½ç½®ä¿¡æ¯

**ä»£ç **:
```python
# [model.py, ç¬¬ 133 è¡Œ]
self.transformer = nn.ModuleDict(dict(
    ...
    wpe = nn.Embedding(config.block_size, config.n_embd),  # position embedding
    ...
))

# [model.py, ç¬¬ 167-168 è¡Œ]
pos = torch.arange(0, t, dtype=torch.long, device=device)
pos_emb = self.transformer.wpe(pos)  # (t, n_embd)
```

### 2. Cosine è¡°å‡å­¦ä¹ ç‡

**åº”ç”¨**: é€šè¿‡ä½™å¼¦å‡½æ•°å¹³æ»‘åœ°è¡°å‡å­¦ä¹ ç‡

**ä»£ç **:
```python
# [train.py, ç¬¬ 24 è¡Œ]
coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))
```

ä½¿ç”¨ $\cos(\pi \cdot x)$ å…¶ä¸­ $x \in [0, 1]$

---

## å¤æ‚åº¦åˆ†æ

### 1. æ—¶é—´å¤æ‚åº¦

**è‡ªæ³¨æ„åŠ›æœºåˆ¶**:
- è®¡ç®— $Q K^T$: $O(B \cdot T^2 \cdot d)$
- Softmax: $O(B \cdot T^2)$
- ä¸ $V$ ç›¸ä¹˜: $O(B \cdot T^2 \cdot d)$

**æ€»**: $O(B \cdot T^2 \cdot d)$

å…¶ä¸­ï¼š
- $B$ = batch size
- $T$ = åºåˆ—é•¿åº¦
- $d$ = åµŒå…¥ç»´åº¦

### 2. ç©ºé—´å¤æ‚åº¦

**æ³¨æ„åŠ›çŸ©é˜µ**: å­˜å‚¨ $O(B \cdot T^2)$ çš„åˆ†æ•°çŸ©é˜µ

**ä»£ç **:
```python
# [model.py, ç¬¬ 51 è¡Œ] æ³¨æ„åŠ›çŸ©é˜µ
att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
```

è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆé•¿åºåˆ—è®­ç»ƒéœ€è¦æ›´å¤š GPU å†…å­˜ã€‚

### 3. æ¨¡å‹å‚æ•°é‡

**è®¡ç®—æ–¹å¼**:

```python
# [model.py, ç¬¬ 183-189 è¡Œ]
def get_num_params(self):
    """è¿”å›æ¨¡å‹å‚æ•°æ€»æ•°"""
    return sum(p.numel() for p in self.parameters())
```

**å‚æ•°åˆ†å¸ƒ**:
- Embedding: $V \cdot d$ ï¼ˆè¯è¡¨å¤§å° Ã— åµŒå…¥ç»´åº¦ï¼‰
- æ¯å±‚æ³¨æ„åŠ›: $3d^2$
- æ¯å±‚ MLP: $8d^2$
- æ€»: $(V \cdot d) + L \cdot (11d^2)$ å…¶ä¸­ $L$ æ˜¯å±‚æ•°

---

## ğŸ“ å…³é”®æ•°å­¦å…¬å¼é€ŸæŸ¥

| æ¦‚å¿µ | å…¬å¼ | ä½ç½® |
|------|------|------|
| ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ› | $\frac{QK^T}{\sqrt{d_k}}$ | model.py:51 |
| Softmax | $\frac{e^{x_i}}{\sum_j e^{x_j}}$ | model.py:54 |
| äº¤å‰ç†µæŸå¤± | $-\sum y_i \log \hat{y}_i$ | model.py:149 |
| Adam æ›´æ–° | $w -= \alpha \cdot m / (\sqrt{v} + \epsilon)$ | train.py:102-108 |
| å­¦ä¹ ç‡è°ƒåº¦ | $\alpha = \alpha_{min} + 0.5(1 + \cos\pi r)(\alpha_{max} - \alpha_{min})$ | train.py:24 |
| å±‚å½’ä¸€åŒ– | $\gamma \frac{x-\mu}{\sqrt{\sigma^2+\epsilon}} + \beta$ | model.py:15 |

---

## ğŸ“ å­¦ä¹ è·¯å¾„å»ºè®®

### åˆçº§
1. çº¿æ€§ä»£æ•°åŸºç¡€ï¼šçŸ©é˜µä¹˜æ³•ã€è½¬ç½®ã€å‘é‡
2. æ¦‚ç‡è®ºåŸºç¡€ï¼šsoftmaxã€äº¤å‰ç†µ
3. å¾®ç§¯åˆ†åŸºç¡€ï¼šå¯¼æ•°ã€é“¾å¼æ³•åˆ™

### ä¸­çº§
4. æ·±åº¦å­¦ä¹ ï¼šåå‘ä¼ æ’­ã€æ¢¯åº¦ä¸‹é™
5. æ³¨æ„åŠ›æœºåˆ¶ï¼šè‡ªæ³¨æ„åŠ›çš„æ•°å­¦åŸç†
6. ä¼˜åŒ–ç®—æ³•ï¼šAdam ä¼˜åŒ–å™¨

### é«˜çº§
7. ä¿¡æ¯è®ºï¼šKL æ•£åº¦ã€äº¤å‰ç†µçš„å…³ç³»
8. æ•°å€¼åˆ†æï¼šç¨³å®šæ€§ã€æ··åˆç²¾åº¦
9. å¤æ‚åº¦åˆ†æï¼šæ—¶é—´å’Œç©ºé—´å¤æ‚åº¦

---

## ğŸ“š å‚è€ƒèµ„æº

1. **Attention Is All You Need** - https://arxiv.org/abs/1706.03762
2. **The Illustrated Transformer** - http://jalammar.github.io/illustrated-transformer/
3. **ç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ ** - http://neuralnetworksanddeeplearning.com/
4. **Understanding LSTM Networks** - http://colah.github.io/posts/2015-08-Understanding-LSTMs/
