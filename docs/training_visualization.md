# ğŸ¬ è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–æŒ‡å—

## ğŸ“º å®æ—¶ç›‘æ§æ£€æŸ¥ç‚¹ç”Ÿæˆ

### åˆ†å±ç›‘æ§æ–¹æ¡ˆï¼ˆæ¨èï¼‰

**ç»ˆç«¯1**: æ‰§è¡Œè®­ç»ƒ
```bash
cd /home/shuwen/llm
python train_cli.py --preset standard --data-file data/zh_sample.txt
```

**ç»ˆç«¯2**: å®æ—¶ç›‘æ§æ£€æŸ¥ç‚¹ç”Ÿæˆ
```bash
# æ–¹æ¡ˆA: æ¯ç§’ç›‘æ§æ–‡ä»¶å˜åŒ–
watch -n 1 'ls -lhtr checkpoints/ | tail -10'

# æ–¹æ¡ˆB: æŒç»­ç›‘æ§ï¼ˆæ›´è¯¦ç»†ï¼‰
while true; do
  echo "=== æ£€æŸ¥ç‚¹çŠ¶æ€ ===" 
  ls -lh checkpoints/*.pt 2>/dev/null | awk '{print $9, "-", $5}'
  echo "=== ä¿®æ”¹æ—¶é—´ ==="
  ls -lt checkpoints/*.pt 2>/dev/null | head -5 | awk '{print $9, "-", $6, $7, $8}'
  sleep 1
done

# æ–¹æ¡ˆC: é’ˆå¯¹æ€§ç›‘æ§ï¼ˆæ¨èï¼‰
while true; do
  clear
  echo "ğŸ“Š æ£€æŸ¥ç‚¹ç›‘æ§ (æ›´æ–°æ—¶é—´: $(date '+%H:%M:%S'))"
  echo "=================================================="
  
  echo -e "\nğŸ† æœ€ä½³æ¨¡å‹:"
  ls -lh checkpoints/best_model.pt 2>/dev/null | awk '{printf "  %s: %s\n", $9, $5}'
  
  echo -e "\nğŸ“Œ æœ€æ–°æ¨¡å‹:"
  ls -lh checkpoints/latest.pt 2>/dev/null | awk '{printf "  %s: %s\n", $9, $5}'
  
  echo -e "\nğŸ“¦ Epochæ£€æŸ¥ç‚¹:"
  ls -lh checkpoints/model_epoch_*.pt 2>/dev/null | awk '{printf "  %s: %s\n", $9, $5}'
  
  echo -e "\nğŸ“ˆ è®­ç»ƒå†å²:"
  if [ -f checkpoints/training_history.json ]; then
    python3 -c "
import json
try:
  with open('checkpoints/training_history.json') as f:
    h = json.load(f)
    if h['epochs']:
      latest_ep = h['epochs'][-1]
      latest_val = h['val_loss'][-1]
      print(f'  Epoch {latest_ep}: val_loss={latest_val:.4f}')
except: pass
    "
  fi
  
  sleep 2
done
```

### æ—¥å¿—ç›‘æ§

**ç»ˆç«¯3**: æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
```bash
# å®æ—¶æŸ¥çœ‹æœ€æ–°çš„è®­ç»ƒæ—¥å¿—
tail -f logs/training_*.log | grep -E "(Epoch|æŸå¤±|æ£€æŸ¥ç‚¹|ä¿å­˜)"

# æˆ–æŸ¥çœ‹ç‰¹å®šæ—¥å¿—
tail -f /home/shuwen/llm/logs/training_$(date +%Y%m%d)*.log
```

## ğŸ“Š å®Œæ•´è®­ç»ƒæµç¨‹æ¼”ç¤º

### ç¬¬1è½® (Epoch 1) - å»ºç«‹åŸºçº¿

```
è®­ç»ƒä¸­:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 2084/6252 [01:29<03:05, 22.33it/s]
        loss=5.1234, avg_loss=5.1340

[1åˆ†é’Ÿå...]

âœ… Epoch 1/3 å®Œæˆ

ğŸ“Š Epoch 1/3 ç»“æœ
==================================================
  â±ï¸  ç”¨æ—¶: 264.1s (4.4min)
  ğŸ“‰ è®­ç»ƒæŸå¤±: 5.2341
  ğŸ“Š éªŒè¯æŸå¤±: 4.8934

ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹...
  âœ“ Epochæ£€æŸ¥ç‚¹: model_epoch_1.pt (487.3MB)
  ğŸ† æœ€ä½³æ¨¡å‹: best_model.pt (487.3MB) [æ”¹è¿›: inf] â† ç¬¬ä¸€æ¬¡å¿…ç„¶æ˜¯æœ€ä½³
  ğŸ“Œ æœ€æ–°æ¨¡å‹: latest.pt (487.3MB) [ç”¨äºæ¢å¤è®­ç»ƒ]
  ğŸ“ ä¸»æ£€æŸ¥ç‚¹: model.pt (487.3MB)
==================================================

ğŸ¯ æ­¤æ—¶ checkpoints/ ç›®å½•:
  âœ“ model_epoch_1.pt (487.3MB) - Epoch 1å®Œæ•´
  âœ“ best_model.pt (487.3MB) - å½“å‰æœ€ä¼˜
  âœ“ latest.pt (487.3MB) - æœ€æ–°ä¿å­˜
  âœ“ model.pt (487.3MB) - ä¸»æ£€æŸ¥ç‚¹
```

### ç¬¬2è½® (Epoch 2) - æ¨¡å‹æ”¹è¿›

```
[4åˆ†é’Ÿå...]

âœ… Epoch 2/3 å®Œæˆ

ğŸ“Š Epoch 2/3 ç»“æœ
==================================================
  â±ï¸  ç”¨æ—¶: 268.3s (4.5min)
  ğŸ“‰ è®­ç»ƒæŸå¤±: 3.9843
  ğŸ“Š éªŒè¯æŸå¤±: 3.8012

ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹...
  âœ“ Epochæ£€æŸ¥ç‚¹: model_epoch_2.pt (487.3MB)
  ğŸ† æœ€ä½³æ¨¡å‹: best_model.pt (487.3MB) [æ”¹è¿›: 1.0922] â† æŸå¤±ä»4.89é™åˆ°3.80
  ğŸ“Œ æœ€æ–°æ¨¡å‹: latest.pt (487.3MB) [ç”¨äºæ¢å¤è®­ç»ƒ]
  ğŸ“ ä¸»æ£€æŸ¥ç‚¹: model.pt (487.3MB)
==================================================

ğŸ¯ æ­¤æ—¶ checkpoints/ ç›®å½•:
  âœ“ model_epoch_1.pt (487.3MB) - Epoch 1
  âœ“ model_epoch_2.pt (487.3MB) - Epoch 2
  âœ“ best_model.pt (487.3MB) - â­ æ›´æ–°ä¸ºEpoch 2
  âœ“ latest.pt (487.3MB) - æ›´æ–°ä¸ºEpoch 2
  âœ“ model.pt (487.3MB) - æ›´æ–°ä¸ºEpoch 2
```

### ç¬¬3è½® (Epoch 3) - æœ€åå†²åˆº

```
[4åˆ†é’Ÿå...]

âœ… Epoch 3/3 å®Œæˆ

ğŸ“Š Epoch 3/3 ç»“æœ
==================================================
  â±ï¸  ç”¨æ—¶: 265.8s (4.4min)
  ğŸ“‰ è®­ç»ƒæŸå¤±: 3.4521
  ğŸ“Š éªŒè¯æŸå¤±: 3.6721

ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹...
  âœ“ Epochæ£€æŸ¥ç‚¹: model_epoch_3.pt (487.3MB)
  â„¹ï¸  éªŒè¯æŸå¤±æœªæ”¹è¿› (æœ€ä½³: 3.8012) â† è¿™æ¬¡æ²¡æœ‰è¶…è¿‡Epoch 2
  ğŸ“Œ æœ€æ–°æ¨¡å‹: latest.pt (487.3MB) [ç”¨äºæ¢å¤è®­ç»ƒ]
  ğŸ“ ä¸»æ£€æŸ¥ç‚¹: model.pt (487.3MB)
==================================================

ğŸ¯ æœ€ç»ˆ checkpoints/ ç›®å½•:
  âœ“ model_epoch_1.pt (487.3MB) - Epoch 1
  âœ“ model_epoch_2.pt (487.3MB) - Epoch 2 â­ æœ€ä¼˜
  âœ“ model_epoch_3.pt (487.3MB) - Epoch 3
  âœ“ best_model.pt (487.3MB) - ğŸ† æœ€ä¼˜æ¨¡å‹ = Epoch 2
  âœ“ latest.pt (487.3MB) - ğŸ“Œ æœ€æ–°æ¨¡å‹ = Epoch 3
  âœ“ model.pt (487.3MB) - ä¸»æ£€æŸ¥ç‚¹ = Epoch 3
  âœ“ training_history.json (248B) - å®Œæ•´å†å²
```

### è®­ç»ƒå®Œæˆæ€»ç»“

```
================================================================================
âœ… è®­ç»ƒå®Œæˆ!
================================================================================
  â±ï¸  æ€»ç”¨æ—¶: 797.2s (13.3min)
  ğŸ“Š æœ€ä½³éªŒè¯æŸå¤±: 3.8012 (Epoch 2)

ğŸ“ ä¿å­˜çš„æ¨¡å‹:
  ğŸ† æœ€ä½³æ¨¡å‹: checkpoints/best_model.pt
  ğŸ“Œ æœ€æ–°æ¨¡å‹: checkpoints/latest.pt
  ğŸ“¦ Epochæ£€æŸ¥ç‚¹: checkpoints/model_epoch_*.pt (æœ€è¿‘3ä¸ª)

ğŸš€ ä½¿ç”¨è®­ç»ƒåçš„æ¨¡å‹:
  # ä½¿ç”¨æœ€ä½³æ¨¡å‹
  LLM_CHECKPOINT=checkpoints/best_model.pt make serve
  # æˆ–ç»§ç»­è®­ç»ƒ
  python train_chinese.py --checkpoint checkpoints/latest.pt --epochs 3
================================================================================

ğŸ“ˆ è®­ç»ƒå†å²å·²ä¿å­˜: checkpoints/training_history.json
```

## ğŸ” æŸ¥çœ‹å†å²çš„3ç§æ–¹å¼

### æ–¹å¼1: å‘½ä»¤è¡ŒæŸ¥çœ‹

```bash
python train_manager.py history
```

è¾“å‡ºï¼š
```
ğŸ“Š è®­ç»ƒå†å²åˆ†æ
================================================================================
æ€»è½®æ•°: 3
æœ€ä½³è½®æ¬¡: 2
æœ€ä½³éªŒè¯æŸå¤±: 3.8012

æŸå¤±æ›²çº¿:
  Epoch 1: ğŸ“ˆ val_loss=4.8934
  Epoch 2: ğŸ“‰ val_loss=3.8012  â­ æœ€ä½³
  Epoch 3: ğŸ“ˆ val_loss=3.6721
================================================================================
```

### æ–¹å¼2: æ£€æŸ¥åˆ—è¡¨

```bash
python train_manager.py list
```

è¾“å‡ºï¼š
```
ğŸ“‹ è®­ç»ƒæ£€æŸ¥ç‚¹
================================================================================
æ–‡ä»¶å              Epoch  éªŒè¯æŸå¤±   å¤§å°     ä¿®æ”¹æ—¶é—´
================================================================================
model_epoch_1.pt      1    4.8934    487.3MB  2026-02-28 17:45:23
model_epoch_2.pt      2    3.8012    487.3MB  2026-02-28 17:50:15 â­
model_epoch_3.pt      3    3.6721    487.3MB  2026-02-28 17:55:07
best_model.pt         2    3.8012    487.3MB  2026-02-28 17:50:15
latest.pt             3    3.6721    487.3MB  2026-02-28 17:55:07
================================================================================
```

### æ–¹å¼3: JSONæŸ¥çœ‹ï¼ˆç”¨äºåˆ†æï¼‰

```bash
cat checkpoints/training_history.json | python -m json.tool
```

è¾“å‡ºï¼š
```json
{
  "train_loss": [5.2341, 3.9843, 3.4521],
  "val_loss": [4.8934, 3.8012, 3.6721],
  "epochs": [1, 2, 3]
}
```

## ğŸ¯ å®æ—¶å¯¹æ¯”ç¤ºä¾‹

### å½“å‰è®­ç»ƒè¿›è¡Œä¸­

```bash
# ç»ˆç«¯1: æ‰§è¡Œè®­ç»ƒ
python train_cli.py --preset extended --data-file data/zh_sample.txt

# ç»ˆç«¯2: ç›‘æ§è¿›åº¦
while true; do
  clear
  echo "ğŸ“Š è®­ç»ƒè¿›åº¦ç›‘æ§ (æ›´æ–°: $(date))"
  
  # æ˜¾ç¤ºå½“å‰åœ¨è¿è¡Œçš„è¿›ç¨‹
  ps aux | grep train_cli | grep -v grep && echo "âœ“ è®­ç»ƒè¿›è¡Œä¸­"
  
  # æ˜¾ç¤ºæœ€æ–°çš„æ—¥å¿—è¡Œ
  echo -e "\nğŸ“ æœ€æ–°æ—¥å¿—:"
  tail -3 logs/training_*.log 2>/dev/null
  
  # æ˜¾ç¤ºå½“å‰çš„æ£€æŸ¥ç‚¹
  echo -e "\nğŸ“ æ£€æŸ¥ç‚¹çŠ¶æ€:"
  ls -lt checkpoints/*.pt 2>/dev/null | head -3 | awk '{print $9, "("$5")", "- ä¿®æ”¹äº", $6, $7, $8}'
  
  sleep 2
done
```

## ğŸ’¡ å…³é”®æ—¶åˆ»æˆªå›¾

### âœ… ç¬¬ä¸€ä¸ªæ£€æŸ¥ç‚¹å‡ºç°ï¼ˆEpoch 1å®Œæˆï¼‰

```
æ—¶é—´æˆ³: 17:45:23
æ–°æ–‡ä»¶å‡ºç°:
  âœ“ checkpoints/model_epoch_1.pt (487.3MB)
  âœ“ checkpoints/best_model.pt (487.3MB)
  âœ“ checkpoints/latest.pt (487.3MB)
  âœ“ checkpoints/model.pt (487.3MB)
```

### ğŸ† æœ€ä½³æ¨¡å‹æ›´æ–°ï¼ˆå½“éªŒè¯æŸå¤±æ”¹è¿›ï¼‰

```
æ—¶é—´æˆ³: 17:50:15
æœ€ä½³æ¨¡å‹æ›´æ–°:
  best_model.pt æ–‡ä»¶ä¿®æ”¹æ—¶é—´æ”¹å˜
  ä» model_epoch_1 â†’ model_epoch_2
  æ”¹è¿›: 4.8934 â†’ 3.8012 (æå‡ 1.0922)
```

### ğŸ“Œ æœ€æ–°æ¨¡å‹æ›´æ–°ï¼ˆæ¯ä¸ªEpochï¼‰

```
æ—¶é—´æˆ³: 17:55:07
æœ€æ–°æ¨¡å‹æ›´æ–°:
  latest.pt æ–‡ä»¶ä¿®æ”¹æ—¶é—´æ”¹å˜
  ä» epoch 2 â†’ epoch 3
  è¿™ä¸ªæ–‡ä»¶åŒ…å«å®Œæ•´çš„ä¼˜åŒ–å™¨çŠ¶æ€
```

## ğŸš¨ æ£€æŸ¥ç‚¹æœªç”Ÿæˆçš„æ’æŸ¥

å¦‚æœä½ çœ‹ä¸åˆ°æ£€æŸ¥ç‚¹ç”Ÿæˆï¼Œå¯èƒ½çš„åŸå› å’Œè§£å†³æ–¹æ¡ˆï¼š

### âŒ é—®é¢˜1: è¾“å‡ºè¢«è¿›åº¦æ¡è¦†ç›–

**ç—‡çŠ¶**: çœ‹ä¸åˆ°"ä¿å­˜æ£€æŸ¥ç‚¹"çš„ä¿¡æ¯

**è§£å†³**:
```bash
# 1. é‡å®šå‘åˆ°æ–‡ä»¶æŸ¥çœ‹
python train_cli.py --preset standard 2>&1 | tee training.log

# 2. åœ¨å¦ä¸€ä¸ªç»ˆç«¯æŸ¥çœ‹æ—¥å¿—
tail -f training.log | grep -E "(ä¿å­˜|æ£€æŸ¥ç‚¹|Epoch)"
```

### âŒ é—®é¢˜2: æ£€æŸ¥ç‚¹ç›®å½•ä¸å­˜åœ¨

**ç—‡çŠ¶**: æŠ¥é”™ "checkpoints ç›®å½•ä¸å­˜åœ¨"

**è§£å†³**:
```bash
# æ‰‹åŠ¨åˆ›å»ºç›®å½•
mkdir -p /home/shuwen/llm/checkpoints

# æ£€æŸ¥æƒé™
chmod 755 /home/shuwen/llm/checkpoints
```

### âŒ é—®é¢˜3: ç£ç›˜ç©ºé—´ä¸è¶³

**ç—‡çŠ¶**: è®­ç»ƒå¼€å§‹ä½†æœªä¿å­˜æ£€æŸ¥ç‚¹

**è§£å†³**:
```bash
# æ£€æŸ¥ç£ç›˜ç©ºé—´
df -h

# æ¸…ç†æ—§æ£€æŸ¥ç‚¹
python train_manager.py clean --keep 2
```

### âŒ é—®é¢˜4: GPUæ˜¾å­˜æº¢å‡ºå¯¼è‡´è®­ç»ƒä¸­æ–­

**ç—‡çŠ¶**: è®­ç»ƒä¸­æ–­ï¼Œæœªå®ŒæˆEpoch 1

**è§£å†³**:
```bash
# ä½¿ç”¨æ›´å°çš„æ‰¹æ¬¡
python train_cli.py --preset quick  # batch_size=2

# æˆ–æ‰‹åŠ¨è®¾ç½®
python train_cli.py --batch-size 1 --epochs 1
```

## ğŸ“ˆ ç»˜åˆ¶æŸå¤±æ›²çº¿ï¼ˆå¯é€‰ï¼‰

### Pythonè„šæœ¬

```python
# plot_history.py
import json
import matplotlib.pyplot as plt

with open('checkpoints/training_history.json') as f:
    history = json.load(f)

plt.figure(figsize=(10, 6))
plt.plot(history['epochs'], history['train_loss'], 'b-o', label='Train Loss')
plt.plot(history['epochs'], history['val_loss'], 'r-s', label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training History')
plt.legend()
plt.grid()
plt.savefig('training_history.png')
print('âœ“ å›¾è¡¨å·²ä¿å­˜: training_history.png')
```

è¿è¡Œï¼š
```bash
python plot_history.py
```

## ğŸ¬ å®Œæ•´ç›‘æ§è„šæœ¬

ä¿å­˜ä¸º `monitor_training.sh`:

```bash
#!/bin/bash
# å®æ—¶ç›‘æ§è®­ç»ƒè¿›åº¦å’Œæ£€æŸ¥ç‚¹ç”Ÿæˆ

CHECKPOINT_DIR="checkpoints"
mkdir -p "$CHECKPOINT_DIR"

echo "ğŸ¬ å¼€å§‹ç›‘æ§è®­ç»ƒ..."
echo "Ctrl+C é€€å‡º"

while true; do
  clear
  
  # æ ‡é¢˜å’Œæ—¶é—´
  echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
  echo "â•‘  ğŸ“ è®­ç»ƒç›‘æ§é¢æ¿ ($(date '+%H:%M:%S'))  â•‘"
  echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
  
  # æ£€æŸ¥è®­ç»ƒè¿›ç¨‹
  if pgrep -f "train_cli\|train_chinese" > /dev/null; then
    echo "âœ“ è®­ç»ƒè¿›è¡Œä¸­..."
  else
    echo "âš ï¸  è®­ç»ƒæœªè¿è¡Œ"
  fi
  
  echo ""
  echo "ğŸ“Š æ£€æŸ¥ç‚¹çŠ¶æ€:"
  echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
  
  # åˆ—å‡ºæœ€æ–°çš„5ä¸ªæ–‡ä»¶
  if [ -d "$CHECKPOINT_DIR" ]; then
    ls -lt "$CHECKPOINT_DIR"/*.pt 2>/dev/null | head -5 | while read -r line; do
      size=$(echo "$line" | awk '{print $5}')
      time=$(echo "$line" | awk '{print $6, $7, $8}')
      file=$(echo "$line" | awk '{print $NF}' | xargs basename)
      printf "  %-25s %10s  %s\n" "$file" "$size" "$time"
    done
  fi
  
  echo ""
  echo "ğŸ“ˆ è®­ç»ƒå†å²:"
  echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
  
  # æ˜¾ç¤ºè®­ç»ƒå†å²
  if [ -f "$CHECKPOINT_DIR/training_history.json" ]; then
    python3 << 'EOF'
import json
try:
    with open('checkpoints/training_history.json') as f:
        h = json.load(f)
        for i, epoch in enumerate(h['epochs'][-3:]):
            tl = h['train_loss'][i-3] if len(h['train_loss']) >= 3 else h['train_loss'][i]
            vl = h['val_loss'][i-3] if len(h['val_loss']) >= 3 else h['val_loss'][i]
            marker = "â­" if vl == min(h['val_loss']) else "  "
            print(f"  Epoch {epoch}: train={tl:.4f}, val={vl:.4f} {marker}")
except:
    pass
EOF
  fi
  
  echo ""
  echo "æŒ‰ Ctrl+C é€€å‡º | æ¯2ç§’åˆ·æ–°ä¸€æ¬¡"
  sleep 2
done
```

ä½¿ç”¨ï¼š
```bash
bash monitor_training.sh
```

---

è¿™æ ·ä½ å°±èƒ½å®Œæ•´åœ°çœ‹åˆ°æ¯ä¸ªæ£€æŸ¥ç‚¹çš„ç”Ÿæˆè¿‡ç¨‹å’Œæ–‡ä»¶å¤§å°å˜åŒ–äº†ï¼
