# LLM é¡¹ç›®ä¸­çš„æ·±åº¦å­¦ä¹ çŸ¥è¯†å…¨è§£æ

æœ¬æ–‡æ¡£è¯¦ç»†åˆ†æè¿™ä¸ªå¤§è¯­è¨€æ¨¡å‹é¡¹ç›®ä¸­æ¶‰åŠçš„æ‰€æœ‰æ·±åº¦å­¦ä¹ æ¦‚å¿µã€æŠ€æœ¯å’Œæœ€ä½³å®è·µã€‚

## ğŸ“‘ ç›®å½•

1. [ç¥ç»ç½‘ç»œåŸºç¡€](#ç¥ç»ç½‘ç»œåŸºç¡€)
2. [Transformeræ¶æ„](#transformeræ¶æ„)
3. [è®­ç»ƒæŠ€æœ¯](#è®­ç»ƒæŠ€æœ¯)
4. [ä¼˜åŒ–ç®—æ³•](#ä¼˜åŒ–ç®—æ³•)
5. [æ­£åˆ™åŒ–æŠ€æœ¯](#æ­£åˆ™åŒ–æŠ€æœ¯)
6. [ç°ä»£æ·±åº¦å­¦ä¹ å®è·µ](#ç°ä»£æ·±åº¦å­¦ä¹ å®è·µ)
7. [è‡ªç„¶è¯­è¨€å¤„ç†](#è‡ªç„¶è¯­è¨€å¤„ç†)

---

## 1. ç¥ç»ç½‘ç»œåŸºç¡€

### 1.1 å…¨è¿æ¥å±‚ (Linear Layer)

**æ¦‚å¿µ**: çº¿æ€§å˜æ¢ $y = Wx + b$

**ä»£ç ä½ç½®**: [model.py](../model.py)
```python
# ç¬¬ 27 è¡Œ
self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)

# ç¬¬ 29 è¡Œ
self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)
```

**ä½œç”¨**:
- å°†è¾“å…¥å‘é‡æ˜ å°„åˆ°ä¸åŒçš„è¡¨ç¤ºç©ºé—´
- å­¦ä¹ ç‰¹å¾ä¹‹é—´çš„çº¿æ€§ç»„åˆå…³ç³»
- å‚æ•°å¯å­¦ä¹ ï¼Œé€šè¿‡åå‘ä¼ æ’­æ›´æ–°

**å‚æ•°é‡è®¡ç®—**:
```
å‚æ•°æ•°é‡ = (è¾“å…¥ç»´åº¦ Ã— è¾“å‡ºç»´åº¦) + è¾“å‡ºç»´åº¦(bias)
ä¾‹: (384 Ã— 1152) + 1152 = 443,520
```

---

### 1.2 æ¿€æ´»å‡½æ•°

#### GELU (Gaussian Error Linear Unit)

**ä½ç½®**: [model.py](../model.py) ç¬¬ 74 è¡Œ

```python
self.gelu = nn.GELU()
```

**æ•°å­¦å®šä¹‰**:
$$\text{GELU}(x) = x \cdot \Phi(x) = x \cdot \frac{1}{2}\left[1 + \text{erf}\left(\frac{x}{\sqrt{2}}\right)\right]$$

**ç‰¹ç‚¹**:
- å¹³æ»‘çš„éçº¿æ€§æ¿€æ´»
- ç±»ä¼¼ReLUä½†æ›´å¹³æ»‘
- åœ¨Transformeræ¨¡å‹ä¸­è¡¨ç°ä¼˜å¼‚
- å¯ä»¥è¿‘ä¼¼ä¸º: $0.5x(1 + \tanh[\sqrt{2/\pi}(x + 0.044715x^3)])$

**ä¸ºä»€ä¹ˆæ¯”ReLUå¥½**:
- æä¾›æ¦‚ç‡æ€§è§£é‡Šï¼ˆéšæœºæ­£åˆ™åŒ–ï¼‰
- æ¢¯åº¦æ›´å¹³æ»‘
- ç»éªŒä¸Šæ•ˆæœæ›´å¥½

---

### 1.3 åµŒå…¥å±‚ (Embedding)

**ä½ç½®**: [model.py](../model.py) ç¬¬ 128-129 è¡Œ

```python
wte = nn.Embedding(config.vocab_size, config.n_embd)  # token embedding
wpe = nn.Embedding(config.block_size, config.n_embd)  # position embedding
```

**æ¦‚å¿µ**:
- **Token Embedding**: å°†ç¦»æ•£çš„è¯æ±‡IDæ˜ å°„åˆ°è¿ç»­å‘é‡ç©ºé—´
- **Position Embedding**: ä¸ºæ¯ä¸ªä½ç½®å­¦ä¹ ä¸€ä¸ªå‘é‡è¡¨ç¤º

**æ•°å­¦**:
```
vocab_size = 50257
embedding_dim = 384
æ¯ä¸ªtoken â†’ 384ç»´å‘é‡
å‚æ•°é‡ = 50257 Ã— 384 = 19,298,688
```

**ä¸ºä»€ä¹ˆéœ€è¦**:
- ç¥ç»ç½‘ç»œåªèƒ½å¤„ç†æ•°å­—
- åµŒå…¥ç©ºé—´ä¸­ç›¸ä¼¼çš„è¯è·ç¦»æ›´è¿‘
- å¯å­¦ä¹ çš„è¡¨ç¤ºæ¯”one-hotæ›´ç´§å‡‘

---

## 2. Transformeræ¶æ„

### 2.1 è‡ªæ³¨æ„åŠ›æœºåˆ¶ (Self-Attention)

**ä½ç½®**: [model.py](../model.py) - `CausalSelfAttention` ç±»

#### æ ¸å¿ƒæ€æƒ³
è®©æ¨¡å‹å­¦ä¹ åºåˆ—ä¸­ä¸åŒä½ç½®ä¹‹é—´çš„å…³ç³»ã€‚

#### æ•°å­¦å…¬å¼

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

**ä»£ç å®ç°**:
```python
# ç¬¬ 51 è¡Œï¼šè®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))

# ç¬¬ 54 è¡Œï¼šåº”ç”¨softmax
att = F.softmax(att, dim=-1)

# ç¬¬ 56 è¡Œï¼šåŠ æƒæ±‚å’Œ
y = att @ v
```

#### è¯¦ç»†æ­¥éª¤

**Step 1: è®¡ç®—Q, K, V**
```python
# ç¬¬ 47 è¡Œ
q, k, v = self.c_attn(x).split(self.n_embd, dim=2)
```
- Query (æŸ¥è¯¢): "æˆ‘åœ¨æ‰¾ä»€ä¹ˆ"
- Key (é”®): "æˆ‘æœ‰ä»€ä¹ˆä¿¡æ¯"
- Value (å€¼): "å®é™…çš„ä¿¡æ¯å†…å®¹"

**Step 2: ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›**
- ç‚¹ç§¯: $QK^T$ è®¡ç®—ç›¸ä¼¼åº¦
- ç¼©æ”¾: é™¤ä»¥ $\sqrt{d_k}$ é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±
- Softmax: è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ

**Step 3: åŠ æƒæ±‚å’Œ**
- ç”¨æ³¨æ„åŠ›æƒé‡å¯¹ValueåŠ æƒ
- å¾—åˆ°ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¡¨ç¤º

#### ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ
- **åŠ¨æ€å…³ç³»**: æ ¹æ®è¾“å…¥å†…å®¹å†³å®šå…³æ³¨ä»€ä¹ˆ
- **é•¿è·ç¦»ä¾èµ–**: å¯ä»¥å…³æ³¨åºåˆ—ä¸­ä»»æ„ä½ç½®
- **å¹¶è¡Œè®¡ç®—**: æ‰€æœ‰ä½ç½®åŒæ—¶å¤„ç†

---

### 2.2 å¤šå¤´æ³¨æ„åŠ› (Multi-Head Attention)

**æ¦‚å¿µ**: å¹¶è¡Œè¿è¡Œå¤šä¸ªæ³¨æ„åŠ›æœºåˆ¶

**ä»£ç **:
```python
# ç¬¬ 49-51 è¡Œï¼šé‡å¡‘ä¸ºå¤šå¤´
k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
```

**å‚æ•°**:
- `n_head = 6`: 6ä¸ªæ³¨æ„åŠ›å¤´
- `n_embd = 384`: æ€»ç»´åº¦
- æ¯ä¸ªå¤´: `384 / 6 = 64` ç»´

**ä¼˜åŠ¿**:
- ä¸åŒçš„å¤´å­¦ä¹ ä¸åŒçš„æ¨¡å¼
- Head 1: å¯èƒ½å…³æ³¨è¯­æ³•
- Head 2: å¯èƒ½å…³æ³¨è¯­ä¹‰
- Head 3: å¯èƒ½å…³æ³¨é•¿è·ç¦»ä¾èµ–
- ç»„åˆåè·å¾—æ›´ä¸°å¯Œçš„è¡¨ç¤º

**å½¢è±¡ç†è§£**:
```
è¾“å…¥æ–‡æœ¬: "The cat sat on the mat"

Head 1 å…³æ³¨: cat â† â†’ sat (ä¸»è°“å…³ç³»)
Head 2 å…³æ³¨: sat â† â†’ on (åŠ¨è¯-ä»‹è¯)
Head 3 å…³æ³¨: on â† â†’ mat (ä»‹è¯-å®¾è¯­)
...
```

---

### 2.3 å› æœæ³¨æ„åŠ› (Causal Attention)

**ä½ç½®**: [model.py](../model.py) ç¬¬ 39-41 è¡Œ

```python
# å› æœmaskï¼ˆä¸‹ä¸‰è§’çŸ©é˜µï¼‰
self.register_buffer("bias", torch.tril(torch.ones(config.block_size, config.block_size))
                    .view(1, 1, config.block_size, config.block_size))

# ç¬¬ 53 è¡Œï¼šåº”ç”¨mask
att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))
```

**æ¦‚å¿µ**: åªèƒ½çœ‹åˆ°å½“å‰ä½ç½®ä¹‹å‰çš„å†…å®¹

**MaskçŸ©é˜µ**:
```
ä½ç½®:  0  1  2  3  4
  0 [  1  0  0  0  0 ]  â† ä½ç½®0åªèƒ½çœ‹åˆ°è‡ªå·±
  1 [  1  1  0  0  0 ]  â† ä½ç½®1èƒ½çœ‹åˆ°0å’Œ1
  2 [  1  1  1  0  0 ]  â† ä½ç½®2èƒ½çœ‹åˆ°0,1,2
  3 [  1  1  1  1  0 ]
  4 [  1  1  1  1  1 ]
```

**ä¸ºä»€ä¹ˆéœ€è¦**:
- **è‡ªå›å½’ç”Ÿæˆ**: é¢„æµ‹ä¸‹ä¸€ä¸ªè¯
- **é˜²æ­¢ä¿¡æ¯æ³„éœ²**: è®­ç»ƒæ—¶ä¸èƒ½çœ‹åˆ°æœªæ¥
- **ä¿æŒå› æœæ€§**: æ¨¡æ‹ŸçœŸå®æ¨ç†è¿‡ç¨‹

---

### 2.4 å‰é¦ˆç½‘ç»œ (Feed-Forward Network)

**ä½ç½®**: [model.py](../model.py) - `MLP` ç±»

```python
class MLP(nn.Module):
    def __init__(self, config):
        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)  # æ‰©å±•
        self.gelu = nn.GELU()                                     # éçº¿æ€§
        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)# å‹ç¼©
        self.dropout = nn.Dropout(config.dropout)
```

**ç»“æ„**: 384 â†’ 1536 â†’ 384

**ä½œç”¨**:
- å¯¹æ¯ä¸ªä½ç½®ç‹¬ç«‹å¤„ç†ï¼ˆposition-wiseï¼‰
- å¢åŠ æ¨¡å‹çš„éçº¿æ€§è¡¨è¾¾èƒ½åŠ›
- ä¸­é—´å±‚æ‰©å±•4å€ï¼ˆå¸¸è§åšæ³•ï¼‰

**ä¸ºä»€ä¹ˆæ‰©å±•4å€**:
- æä¾›æ›´å¤§çš„è¡¨ç¤ºç©ºé—´
- å¢åŠ æ¨¡å‹å®¹é‡
- ç»éªŒä¸Šæ•ˆæœå¥½

---

### 2.5 æ®‹å·®è¿æ¥ (Residual Connections)

**ä½ç½®**: [model.py](../model.py) ç¬¬ 95-96 è¡Œ

```python
def forward(self, x):
    x = x + self.attn(self.ln_1(x))  # æ®‹å·®è¿æ¥
    x = x + self.mlp(self.ln_2(x))   # æ®‹å·®è¿æ¥
```

**æ•°å­¦**: $y = x + F(x)$

**ä¸ºä»€ä¹ˆé‡è¦**:
1. **æ¢¯åº¦æµåŠ¨**: æä¾›æ¢¯åº¦çš„ç›´æ¥é€šè·¯
2. **è®­ç»ƒæ·±å±‚ç½‘ç»œ**: é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±
3. **æ’ç­‰æ˜ å°„**: è‡³å°‘èƒ½å­¦åˆ°æ’ç­‰å‡½æ•°
4. **ç‰¹å¾é‡ç”¨**: ä¿ç•™åŸå§‹ä¿¡æ¯

**å¯è§†åŒ–**:
```
è¾“å…¥ x
  â†“
  â”œâ†’ LayerNorm â†’ Attention â†’ +
  â†“                           â†“
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
  â”œâ†’ LayerNorm â†’ MLP â†’ +
  â†“                     â†“
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
è¾“å‡º
```

---

### 2.6 å±‚å½’ä¸€åŒ– (Layer Normalization)

**ä½ç½®**: [model.py](../model.py) - `LayerNorm` ç±»

```python
def forward(self, input):
    return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)
```

**æ•°å­¦**:
$$y = \gamma \cdot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta$$

å…¶ä¸­:
- $\mu$ = å‡å€¼
- $\sigma^2$ = æ–¹å·®
- $\gamma, \beta$ = å¯å­¦ä¹ å‚æ•°
- $\epsilon = 10^{-5}$ = æ•°å€¼ç¨³å®šæ€§å¸¸æ•°

**ä½œç”¨**:
- ç¨³å®šè®­ç»ƒ
- åŠ é€Ÿæ”¶æ•›
- å‡å°‘å†…éƒ¨åå˜é‡åç§»

**LayerNorm vs BatchNorm**:
```
BatchNorm:  æ²¿batchç»´åº¦å½’ä¸€åŒ– (é€‚åˆCNN)
LayerNorm:  æ²¿ç‰¹å¾ç»´åº¦å½’ä¸€åŒ– (é€‚åˆRNN/Transformer)
```

---

## 3. è®­ç»ƒæŠ€æœ¯

### 3.1 åå‘ä¼ æ’­ (Backpropagation)

**ä½ç½®**: [train.py](../train.py) ç¬¬ 121-124 è¡Œ

```python
# å‰å‘ä¼ æ’­
logits, loss = model(x, y)

# åå‘ä¼ æ’­
optimizer.zero_grad(set_to_none=True)
loss.backward()
```

**åŸç†**:
- ä½¿ç”¨é“¾å¼æ³•åˆ™è®¡ç®—æ¢¯åº¦
- ä»è¾“å‡ºå±‚åˆ°è¾“å…¥å±‚åå‘è®¡ç®—
- PyTorchè‡ªåŠ¨å¾®åˆ†ï¼ˆautogradï¼‰

**æ•°å­¦**:
$$\frac{\partial L}{\partial w} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial w}$$

---

### 3.2 æŸå¤±å‡½æ•°ï¼šäº¤å‰ç†µ

**ä½ç½®**: [model.py](../model.py) ç¬¬ 149 è¡Œ

```python
loss = F.cross_entropy(logits.view(-1, logits.size(-1)), 
                       targets.view(-1), 
                       ignore_index=-1)
```

**æ•°å­¦**:
$$L = -\frac{1}{N}\sum_{i=1}^{N} \sum_{c=1}^{C} y_{i,c} \log(\hat{y}_{i,c})$$

**è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨**:
- é¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡åˆ†å¸ƒ
- $C$ = vocab_size (50257)
- æœ€å°åŒ–çœŸå®åˆ†å¸ƒå’Œé¢„æµ‹åˆ†å¸ƒçš„å·®å¼‚

**ä¸ºä»€ä¹ˆç”¨äº¤å‰ç†µ**:
- é€‚åˆåˆ†ç±»é—®é¢˜
- æ¦‚ç‡è§£é‡Šæ¸…æ™°
- æ¢¯åº¦æ€§è´¨å¥½

---

### 3.3 æ‰¹å¤„ç† (Batching)

**ä½ç½®**: [data.py](../data.py)

```python
def create_dataloader(dataset, batch_size, shuffle=True):
    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)
```

**æ¦‚å¿µ**: ä¸€æ¬¡å¤„ç†å¤šä¸ªæ ·æœ¬

**é…ç½®**: `batch_size = 16`

**ä¼˜åŠ¿**:
- **è®¡ç®—æ•ˆç‡**: GPUå¹¶è¡Œå¤„ç†
- **æ¢¯åº¦ä¼°è®¡**: æ›´ç¨³å®šçš„æ›´æ–°æ–¹å‘
- **å†…å­˜åˆ©ç”¨**: å……åˆ†åˆ©ç”¨ç¡¬ä»¶
- **æ”¶æ•›é€Ÿåº¦**: æ›´å¿«åˆ°è¾¾æœ€ä¼˜è§£

**æƒè¡¡**:
- å¤ªå°: è®­ç»ƒæ…¢ï¼Œæ¢¯åº¦å™ªå£°å¤§
- å¤ªå¤§: å†…å­˜ä¸è¶³ï¼Œæ³›åŒ–èƒ½åŠ›å·®
- ç»éªŒå€¼: 16-512

---

### 3.4 æ•°æ®åŠ è½½ä¸é¢„å¤„ç†

**TokenåŒ–**:
```python
# [data.py]
tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')
tokens = tokenizer(text)
```

**åºåˆ—æ‰“åŒ…**:
```python
# [data.py] å›ºå®šé•¿åº¦åºåˆ—
x = tokens[idx:idx + block_size]
y = tokens[idx + 1:idx + 1 + block_size]
```

**ä¸ºä»€ä¹ˆè¿™æ ·åš**:
- å›ºå®šé•¿åº¦ä¾¿äºæ‰¹å¤„ç†
- yæ˜¯xçš„ä¸‹ä¸€ä¸ªtokenï¼ˆè‡ªå›å½’ï¼‰
- æœ€å¤§åŒ–GPUåˆ©ç”¨ç‡

---

## 4. ä¼˜åŒ–ç®—æ³•

### 4.1 Adam ä¼˜åŒ–å™¨

**ä½ç½®**: [train.py](../train.py) ç¬¬ 102-108 è¡Œ

```python
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=train_config.learning_rate,      # 3e-4
    betas=(train_config.beta1, train_config.beta2),  # (0.9, 0.95)
    weight_decay=train_config.weight_decay  # 0.1
)
```

#### Adamç®—æ³•æ ¸å¿ƒ

**æ›´æ–°è§„åˆ™**:
```
m_t = Î²â‚ * m_{t-1} + (1-Î²â‚) * g_t          # ä¸€é˜¶åŠ¨é‡
v_t = Î²â‚‚ * v_{t-1} + (1-Î²â‚‚) * g_tÂ²        # äºŒé˜¶åŠ¨é‡
mÌ‚_t = m_t / (1 - Î²â‚^t)                    # åå·®ä¿®æ­£
vÌ‚_t = v_t / (1 - Î²â‚‚^t)
Î¸_t = Î¸_{t-1} - Î± * mÌ‚_t / (âˆšvÌ‚_t + Îµ)    # å‚æ•°æ›´æ–°
```

**å‚æ•°è¯´æ˜**:
- `Î²â‚ = 0.9`: ä¸€é˜¶åŠ¨é‡è¡°å‡ç‡
- `Î²â‚‚ = 0.95`: äºŒé˜¶åŠ¨é‡è¡°å‡ç‡
- `Î± = 3e-4`: å­¦ä¹ ç‡
- `Îµ = 1e-8`: æ•°å€¼ç¨³å®šæ€§

#### AdamW vs Adam

**AdamW** = Adam + è§£è€¦æƒé‡è¡°å‡

**æƒé‡è¡°å‡** (Weight Decay):
```python
weight_decay = 0.1
# ç­‰ä»·äº L2 æ­£åˆ™åŒ–ï¼Œä½†å®ç°æ–¹å¼ä¸åŒ
```

**ä¸ºä»€ä¹ˆç”¨AdamW**:
- æ›´å¥½çš„æ³›åŒ–æ€§èƒ½
- æ­£ç¡®çš„æƒé‡è¡°å‡å®ç°
- Transformeræ¨¡å‹çš„æ ‡å‡†é€‰æ‹©

---

### 4.2 å­¦ä¹ ç‡è°ƒåº¦

**ä½ç½®**: [train.py](../train.py) - `get_lr()` å‡½æ•°

#### ä¸‰é˜¶æ®µç­–ç•¥

**é˜¶æ®µ1: çº¿æ€§Warmup** (0 â†’ 100 æ­¥)
```python
if it < config.warmup_iters:
    return config.learning_rate * it / config.warmup_iters
```
- ä»0çº¿æ€§å¢åŠ åˆ°æœ€å¤§å€¼
- é˜²æ­¢è®­ç»ƒåˆæœŸçš„ä¸ç¨³å®š

**é˜¶æ®µ2: Cosineè¡°å‡** (100 â†’ 10000 æ­¥)
```python
decay_ratio = (it - warmup) / (max - warmup)
coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))
return min_lr + coeff * (max_lr - min_lr)
```
- å¹³æ»‘åœ°é™ä½å­¦ä¹ ç‡
- Cosineæ›²çº¿ä¸‹é™

**é˜¶æ®µ3: æœ€å°å­¦ä¹ ç‡** (10000+ æ­¥)
```python
if it > lr_decay_iters:
    return min_lr  # 3e-5
```

**å¯è§†åŒ–**:
```
LR
 ^
 |    /â€¾â€¾â€¾\___
 |   /        \___
 |  /             \___________
 | /                          
 +--------------------------------> æ­¥æ•°
   warmup   cosine decay    min
```

**ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡**:
- **Warmup**: ç¨³å®šè®­ç»ƒåˆæœŸ
- **Cosine**: æ¢ç´¢â†’ç²¾ç»†è°ƒä¼˜
- **æœ€å°å€¼**: é˜²æ­¢å­¦ä¹ åœæ»

---

### 4.3 æ¢¯åº¦è£å‰ª (Gradient Clipping)

**ä½ç½®**: [train.py](../train.py) ç¬¬ 126-128 è¡Œ

```python
if train_config.grad_clip != 0.0:
    torch.nn.utils.clip_grad_norm_(model.parameters(), train_config.grad_clip)
```

**é…ç½®**: `grad_clip = 1.0`

**æ•°å­¦**:
$$\text{if } \|\mathbf{g}\| > \text{threshold}: \quad \mathbf{g} \leftarrow \frac{\text{threshold}}{\|\mathbf{g}\|} \mathbf{g}$$

**ä¸ºä»€ä¹ˆéœ€è¦**:
- **é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸**: RNN/Transformerå®¹æ˜“å‡ºç°
- **ç¨³å®šè®­ç»ƒ**: é¿å…å‚æ•°å‰§çƒˆå˜åŒ–
- **ä¿æŒæ–¹å‘**: åªç¼©æ”¾å¤§å°ï¼Œä¸æ”¹å˜æ–¹å‘

---

## 5. æ­£åˆ™åŒ–æŠ€æœ¯

### 5.1 Dropout

**ä½ç½®**: [model.py](../model.py)

```python
# ç¬¬ 35-36 è¡Œï¼šæ³¨æ„åŠ›dropout
self.attn_dropout = nn.Dropout(config.dropout)
self.resid_dropout = nn.Dropout(config.dropout)

# ç¬¬ 76 è¡Œï¼šMLP dropout
self.dropout = nn.Dropout(config.dropout)
```

**é…ç½®**: `dropout = 0.1` (10%)

**å·¥ä½œåŸç†**:
- è®­ç»ƒæ—¶: éšæœºä¸¢å¼ƒ10%çš„ç¥ç»å…ƒ
- æµ‹è¯•æ—¶: ä½¿ç”¨æ‰€æœ‰ç¥ç»å…ƒï¼Œè¾“å‡ºÃ—0.9

**æ•°å­¦**:
$$y = \begin{cases} 
0 & \text{with probability } p \\
\frac{x}{1-p} & \text{with probability } 1-p
\end{cases}$$

**ä¸ºä»€ä¹ˆæœ‰æ•ˆ**:
- é˜²æ­¢è¿‡æ‹Ÿåˆ
- æ¨¡æ‹Ÿé›†æˆå­¦ä¹ 
- ä¿ƒè¿›ç‰¹å¾ç‹¬ç«‹æ€§
- å¢åŠ æ¨¡å‹é²æ£’æ€§

---

### 5.2 æƒé‡è¡°å‡ (Weight Decay)

**ä½ç½®**: [train.py](../train.py)

```python
weight_decay=0.1
```

**æ•°å­¦**: L2æ­£åˆ™åŒ–
$$L_{\text{total}} = L_{\text{loss}} + \lambda \sum_{i} w_i^2$$

**ä½œç”¨**:
- æƒ©ç½šå¤§æƒé‡
- é¼“åŠ±ç®€å•æ¨¡å‹
- æé«˜æ³›åŒ–èƒ½åŠ›

---

### 5.3 æå‰åœæ­¢ (Early Stopping)

**ä½ç½®**: [train.py](../train.py) ç¬¬ 138-147 è¡Œ

```python
if val_loss < best_val_loss:
    best_val_loss = val_loss
    checkpoint = {...}
    torch.save(checkpoint, 'best_model.pt')
```

**ç­–ç•¥**:
- æ¯500æ­¥è¯„ä¼°éªŒè¯é›†
- ä¿å­˜æœ€ä½³æ¨¡å‹
- é˜²æ­¢è¿‡æ‹Ÿåˆ

---

## 6. ç°ä»£æ·±åº¦å­¦ä¹ å®è·µ

### 6.1 æ··åˆç²¾åº¦è®­ç»ƒ (Mixed Precision)

**ä½ç½®**: [train.py](../train.py) ç¬¬ 116-117 è¡Œ

```python
with torch.amp.autocast(device_type='cuda', dtype=torch.float16):
    logits, loss = model(x, y)
```

**æ¦‚å¿µ**:
- éƒ¨åˆ†è®¡ç®—ä½¿ç”¨float16
- æ•æ„Ÿæ“ä½œä½¿ç”¨float32
- ä¿æŒæ•°å€¼ç¨³å®šæ€§

**ä¼˜åŠ¿**:
- **é€Ÿåº¦**: 2-3å€åŠ é€Ÿ
- **å†…å­˜**: å‡å°‘50%æ˜¾å­˜
- **ç²¾åº¦**: å‡ ä¹æ— æŸå¤±

**å®ç°**:
```
å‰å‘ä¼ æ’­: float16 (å¿«é€Ÿ)
æ¢¯åº¦è®¡ç®—: float16
æ¢¯åº¦ç´¯ç§¯: float32 (ç²¾ç¡®)
å‚æ•°æ›´æ–°: float32
```

---

### 6.2 æƒé‡åˆå§‹åŒ–

**ä½ç½®**: [model.py](../model.py) ç¬¬ 165-176 è¡Œ

```python
def _init_weights(self, module):
    if isinstance(module, nn.Linear):
        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
```

**ç­–ç•¥**:
- **æ­£æ€åˆ†å¸ƒ**: $N(0, 0.02^2)$
- **æ®‹å·®ç¼©æ”¾**: $\text{std} = \frac{0.02}{\sqrt{2 \times n\_layer}}$

**ä¸ºä»€ä¹ˆé‡è¦**:
- å½±å“è®­ç»ƒç¨³å®šæ€§
- å½±å“æ”¶æ•›é€Ÿåº¦
- GPT-2çš„ç»éªŒå€¼

---

### 6.3 æ¢¯åº¦ç´¯ç§¯

è™½ç„¶ç°åœ¨æ²¡å®ç°ï¼Œä½†è¿™æ˜¯å¸¸è§æŠ€æœ¯ï¼š

```python
# ä¼ªä»£ç 
for i in range(accumulation_steps):
    loss = model(x, y) / accumulation_steps
    loss.backward()
optimizer.step()
```

**ä½œç”¨**: æ¨¡æ‹Ÿæ›´å¤§çš„batch_size

---

### 6.4 æ¨¡å‹ç¼–è¯‘ (torch.compile)

**ä½ç½®**: [train.py](../train.py) ç¬¬ 113-115 è¡Œ

```python
if train_config.compile:
    model = torch.compile(model)
```

**ç‰¹æ€§** (PyTorch 2.0+):
- å³æ—¶ç¼–è¯‘ä¼˜åŒ–
- 10-50%åŠ é€Ÿ
- é›¶ä»£ç æ”¹åŠ¨

---

## 7. è‡ªç„¶è¯­è¨€å¤„ç†

### 7.1 è‡ªå›å½’è¯­è¨€å»ºæ¨¡

**æ¦‚å¿µ**: é¢„æµ‹ä¸‹ä¸€ä¸ªè¯

**æ•°å­¦**:
$$P(w_1, w_2, ..., w_n) = \prod_{i=1}^{n} P(w_i | w_1, ..., w_{i-1})$$

**å®ç°**: [model.py](../model.py)
```python
# è¾“å…¥: "The cat sat"
# é¢„æµ‹: "on"
# ç›®æ ‡: æœ€å¤§åŒ– P("on" | "The cat sat")
```

---

### 7.2 TokenåŒ– (Tokenization)

**ä½ç½®**: [data.py](../data.py)

```python
tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')
```

**ç±»å‹**: BPE (Byte Pair Encoding)

**ç¤ºä¾‹**:
```
"æ·±åº¦å­¦ä¹ " â†’ ["æ·±", "åº¦", "å­¦", "ä¹ "]
"machine" â†’ ["mach", "ine"]
```

**è¯è¡¨å¤§å°**: 50,257

---

### 7.3 æ–‡æœ¬ç”Ÿæˆç­–ç•¥

**ä½ç½®**: [generate.py](../generate.py)

#### è´ªå¿ƒè§£ç 
```python
idx_next = torch.argmax(probs, dim=-1)
```
- æ¯æ¬¡é€‰æ¦‚ç‡æœ€é«˜çš„
- ç¡®å®šæ€§ï¼Œä½†å¯èƒ½é‡å¤

#### Top-ké‡‡æ ·
```python
v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
logits[logits < v[:, [-1]]] = -float('Inf')
probs = F.softmax(logits, dim=-1)
idx_next = torch.multinomial(probs, num_samples=1)
```
- ä»å‰kä¸ªä¸­éšæœºé€‰
- æ›´å¤šæ ·æ€§

#### Temperatureé‡‡æ ·
```python
logits = logits / temperature
```
- temperature < 1: æ›´ä¿å®ˆ
- temperature > 1: æ›´éšæœº

---

## ğŸ“Š çŸ¥è¯†åœ°å›¾

```
æ·±åº¦å­¦ä¹ çŸ¥è¯†ä½“ç³»
â”‚
â”œâ”€â”€ åŸºç¡€ç»„ä»¶
â”‚   â”œâ”€â”€ å…¨è¿æ¥å±‚
â”‚   â”œâ”€â”€ æ¿€æ´»å‡½æ•° (GELU)
â”‚   â”œâ”€â”€ åµŒå…¥å±‚
â”‚   â””â”€â”€ å½’ä¸€åŒ– (LayerNorm)
â”‚
â”œâ”€â”€ Transformeræ¶æ„
â”‚   â”œâ”€â”€ è‡ªæ³¨æ„åŠ›æœºåˆ¶
â”‚   â”œâ”€â”€ å¤šå¤´æ³¨æ„åŠ›
â”‚   â”œâ”€â”€ å› æœmask
â”‚   â”œâ”€â”€ å‰é¦ˆç½‘ç»œ
â”‚   â””â”€â”€ æ®‹å·®è¿æ¥
â”‚
â”œâ”€â”€ è®­ç»ƒæŠ€æœ¯
â”‚   â”œâ”€â”€ åå‘ä¼ æ’­
â”‚   â”œâ”€â”€ äº¤å‰ç†µæŸå¤±
â”‚   â”œâ”€â”€ æ‰¹å¤„ç†
â”‚   â””â”€â”€ æ•°æ®åŠ è½½
â”‚
â”œâ”€â”€ ä¼˜åŒ–æ–¹æ³•
â”‚   â”œâ”€â”€ Adam/AdamW
â”‚   â”œâ”€â”€ å­¦ä¹ ç‡è°ƒåº¦
â”‚   â”‚   â”œâ”€â”€ Warmup
â”‚   â”‚   â””â”€â”€ Cosine Decay
â”‚   â””â”€â”€ æ¢¯åº¦è£å‰ª
â”‚
â”œâ”€â”€ æ­£åˆ™åŒ–
â”‚   â”œâ”€â”€ Dropout
â”‚   â”œâ”€â”€ æƒé‡è¡°å‡
â”‚   â””â”€â”€ æå‰åœæ­¢
â”‚
â”œâ”€â”€ ç°ä»£å®è·µ
â”‚   â”œâ”€â”€ æ··åˆç²¾åº¦è®­ç»ƒ
â”‚   â”œâ”€â”€ æƒé‡åˆå§‹åŒ–
â”‚   â””â”€â”€ æ¨¡å‹ç¼–è¯‘
â”‚
â””â”€â”€ NLPç‰¹å®š
    â”œâ”€â”€ è‡ªå›å½’å»ºæ¨¡
    â”œâ”€â”€ TokenåŒ–
    â””â”€â”€ æ–‡æœ¬ç”Ÿæˆ
```

---

## ğŸ“ å­¦ä¹ è·¯å¾„å»ºè®®

### åˆçº§ (ç†è§£åŸºç¡€)
1. ç¥ç»ç½‘ç»œåŸºç¡€ â†’ å…¨è¿æ¥å±‚ã€æ¿€æ´»å‡½æ•°
2. åå‘ä¼ æ’­ â†’ æ¢¯åº¦ä¸‹é™
3. æŸå¤±å‡½æ•° â†’ äº¤å‰ç†µ

### ä¸­çº§ (æŒæ¡æ¶æ„)
4. æ³¨æ„åŠ›æœºåˆ¶ â†’ Transformer
5. æ®‹å·®è¿æ¥ â†’ æ·±å±‚ç½‘ç»œ
6. å½’ä¸€åŒ–æŠ€æœ¯ â†’ LayerNorm

### é«˜çº§ (ä¼˜åŒ–è®­ç»ƒ)
7. ä¼˜åŒ–ç®—æ³• â†’ Adam, AdamW
8. å­¦ä¹ ç‡è°ƒåº¦ â†’ Warmup + Cosine
9. æ­£åˆ™åŒ– â†’ Dropout, Weight Decay

### ä¸“å®¶çº§ (ç°ä»£å®è·µ)
10. æ··åˆç²¾åº¦è®­ç»ƒ
11. åˆ†å¸ƒå¼è®­ç»ƒ
12. æ¨¡å‹å‹ç¼©ä¸é‡åŒ–

---

## ğŸ“š æ¨èèµ„æº

### åŸºç¡€
- **Deep Learning Book** - Ian Goodfellow
- **Neural Networks and Deep Learning** - Michael Nielsen
- **3Blue1Brown** - ç¥ç»ç½‘ç»œå¯è§†åŒ–

### Transformer
- **Attention Is All You Need** - Vaswani et al.
- **The Illustrated Transformer** - Jay Alammar
- **The Annotated Transformer** - Harvard NLP

### å®è·µ
- **nanoGPT** - Andrej Karpathy
- **PyTorchå®˜æ–¹æ–‡æ¡£**
- **Papers with Code**

---

## ğŸ”¥ çƒ­é—¨è¯é¢˜å¯¹åº”

| è¯é¢˜ | æœ¬é¡¹ç›®ä¸­çš„ä½“ç° |
|------|---------------|
| **æ³¨æ„åŠ›æœºåˆ¶** | CausalSelfAttentionç±» |
| **æ®‹å·®ç½‘ç»œ** | Blockç±»ä¸­çš„æ®‹å·®è¿æ¥ |
| **Adamä¼˜åŒ–å™¨** | train.pyä¸­çš„AdamW |
| **Dropout** | æ¨¡å‹å„å±‚ä¸­çš„dropout |
| **æ··åˆç²¾åº¦** | autocaståŠ é€Ÿè®­ç»ƒ |
| **å­¦ä¹ ç‡è°ƒåº¦** | warmup + cosine decay |
| **è‡ªå›å½’æ¨¡å‹** | GPTçš„æ ¸å¿ƒå»ºæ¨¡æ–¹å¼ |

---

## ğŸ’¡ æ€»ç»“

è¿™ä¸ª30Må‚æ•°çš„LLMé¡¹ç›®è™½ç„¶å°å·§ï¼Œä½†åŒ…å«äº†ç°ä»£æ·±åº¦å­¦ä¹ çš„**æ ¸å¿ƒæŠ€æœ¯**ï¼š

âœ… **æ¶æ„**: Transformer (æ³¨æ„åŠ›ã€æ®‹å·®ã€å½’ä¸€åŒ–)  
âœ… **ä¼˜åŒ–**: Adam + å­¦ä¹ ç‡è°ƒåº¦ + æ¢¯åº¦è£å‰ª  
âœ… **æ­£åˆ™åŒ–**: Dropout + æƒé‡è¡°å‡  
âœ… **å·¥ç¨‹**: æ··åˆç²¾åº¦ + æ‰¹å¤„ç† + æ£€æŸ¥ç‚¹  

æŒæ¡è¿™äº›çŸ¥è¯†ï¼Œä½ å°±ç†è§£äº†ç°ä»£å¤§è¯­è¨€æ¨¡å‹çš„**æ ¸å¿ƒåŸç†**ï¼ğŸš€
