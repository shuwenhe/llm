# LLM å·¥ä¸šçº§è®­ç»ƒç³»ç»Ÿ

ä¸€ä¸ªå®Œæ•´çš„ã€OpenAIé£æ ¼çš„ã€å·¥ä¸šçº§çš„å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒç³»ç»Ÿã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

**ğŸ‘‹ é¦–æ¬¡ä½¿ç”¨ï¼Ÿä»è¿™é‡Œå¼€å§‹ï¼š** [docs/START_HERE.md](docs/START_HERE.md)

```bash
python train_cli.py --preset quick
```

## ğŸŒŸ æ ¸å¿ƒç‰¹æ€§

- ğŸ”§ **å®Œæ•´å®ç°**: ä»æ•°æ®å¤„ç†åˆ°æ¨¡å‹è®­ç»ƒçš„å®Œæ•´æµç¨‹
- ğŸ¯ **GPTæ¶æ„**: åŸºäºTransformerçš„è‡ªå›å½’è¯­è¨€æ¨¡å‹
- ğŸ“Š **å¯é…ç½®**: çµæ´»çš„æ¨¡å‹å’Œè®­ç»ƒé…ç½®
- ğŸš€ **æ˜“ç”¨**: ç®€æ´çš„APIå’Œæ¸…æ™°çš„ä»£ç ç»“æ„
- ğŸ’¡ **æ•™è‚²æ€§**: æ³¨é‡Šè¯¦å°½ï¼Œé€‚åˆå­¦ä¹ 
- â­ **å·¥ä¸šçº§**: OpenAIé£æ ¼çš„å‘½ä»¤è¡Œ + å®Œæ•´æ–‡æ¡£

## ğŸ“ é¡¹ç›®ç»“æ„

```
llm/
â”œâ”€ æ ¸å¿ƒæ–‡ä»¶
â”œâ”€ config.py          # æ¨¡å‹å’Œè®­ç»ƒé…ç½®
â”œâ”€ model.py           # GPTæ¨¡å‹å®ç°
â”œâ”€ data.py            # æ•°æ®åŠ è½½å’Œå¤„ç†
â”œâ”€ train.py           # è®­ç»ƒè„šæœ¬
â”œâ”€ generate.py        # æ–‡æœ¬ç”Ÿæˆè„šæœ¬
â”œâ”€ test_model.py      # æ¨¡å‹æµ‹è¯•
â”œâ”€ requirements.txt   # ä¾èµ–ç¨‹åº
â”œâ”€ Makefile           # MakeåŠ é€Ÿå‘½ä»¤
â”œâ”€ setup.sh           # è‡ªåŠ¨è®¾ç½®è„šæœ¬
â”œâ”€ .gitignore         # Gitå¿½ç•¥é…ç½®
â”œâ”€ README.md          # é¡¹ç›®è½»è½¬
â”œâ”€
â”œâ”€ æ–‡æ¡£
â”œâ”€ docs/              # è¯¦ç»†æ–‡æ¡£
â”‚  â”œâ”€ INSTALL.md       # å®‰è£…æŒ‡å—ï¼ˆæ¨ªè·¨ Linux/macOS/Windowsï¼‰
â”‚  â”œâ”€ MATHEMATICS.md    # æ•°å­¦çŸ¥è¯†åˆ†æ
â”‚  â””â”€ VENV_ISSUE.md     # è™šæ‹Ÿç¯å¢ƒé—®é¢˜è§£å†³
â”œâ”€
â”œâ”€ æ•°æ®ä¸æ¨¡å‹
â”œâ”€ checkpoints/      # æ¨¡å‹ä¸‹è½½ä½ç½®ï¼ˆè‡ªåŠ¨åˆ›å»ºï¼‰
â”œâ”€ data/              # æ•°æ®ä¸‹è½½ä½ç½®ï¼ˆè‡ªåŠ¨åˆ›å»ºï¼‰
â”œâ”€ logs/              # è®­ç»ƒæ—¥å¦‹ï¼ˆè‡ªåŠ¨åˆ›å»ºï¼‰
```

## ğŸš€ å¿«é€Ÿå¼€å§‹
> ğŸ’¡ **é‡åˆ°å®‰è£…é—®é¢˜ï¼Ÿ** æŸ¥çœ‹è¯¦ç»†çš„ [å®‰è£…æŒ‡å— (INSTALL.md)](docs/INSTALL.md)


### æ–¹å¼ä¸€ï¼šä½¿ç”¨ Makefileï¼ˆæ¨èï¼‰

```bash
# ä¸€é”®è®¾ç½®ï¼ˆåˆ›å»ºè™šæ‹Ÿç¯å¢ƒå¹¶å®‰è£…æ‰€æœ‰ä¾èµ–ï¼‰
make setup-all

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source venv/bin/activate  # Linux/Mac
# æˆ– venv\Scripts\activate  # Windows

# æµ‹è¯•æ¨¡å‹
make test

# è®­ç»ƒæ¨¡å‹
make train

# å®Œæ•´å¤šæ¨¡æ€è®­ç»ƒ
make train-multimodal

# å¯åŠ¨å·¥ä¸šåŒ–æ¨ç†API
make serve

# ç”Ÿæˆæ–‡æœ¬
make generate
```

**æˆ–è€…åˆ†æ­¥æ‰§è¡Œï¼š**

```bash
# 1. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
make setup

# 2. æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source venv/bin/activate  # Linux/Mac

# 3. å®‰è£…ä¾èµ–
make install

# 4. æŸ¥çœ‹æ‰€æœ‰å¯ç”¨å‘½ä»¤
make help
```

### æ–¹å¼äºŒï¼šç›´æ¥è¿è¡ŒPythonè„šæœ¬

#### 1. å®‰è£…ä¾èµ–

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰
python -m venv venv
source venv/bin/activate  # Linux/Mac
# æˆ– venv\Scripts\activate  # Windows

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

#### 2. è®­ç»ƒæ¨¡å‹

```bash
python train.py
```

è®­ç»ƒå°†ä½¿ç”¨WikiText-2æ•°æ®é›†ï¼Œæ¨¡å‹checkpointä¼šä¿å­˜åœ¨ `checkpoints/` ç›®å½•ã€‚

#### 3. ç”Ÿæˆæ–‡æœ¬

è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿›è¡Œæ–‡æœ¬ç”Ÿæˆï¼š

```bash
python generate.py
```

## ğŸ”§ Makefile å‘½ä»¤å‚è€ƒ

| å‘½ä»¤ | è¯´æ˜ |
|------|------|
| `make help` | æ˜¾ç¤ºæ‰€æœ‰å¯ç”¨å‘½ä»¤ |
| **ç¯å¢ƒè®¾ç½®** | |
| `make setup-all` | â­ ä¸€é”®è®¾ç½®ï¼ˆåˆ›å»ºè™šæ‹Ÿç¯å¢ƒ+å®‰è£…ä¾èµ–ï¼‰ |
| `make setup` | ä»…åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ |
| `make install` | å®‰è£…ä¾èµ–ï¼ˆéœ€è¦å…ˆæ¿€æ´»è™šæ‹Ÿç¯å¢ƒï¼‰ |
| `make install-force` | å¼ºåˆ¶å®‰è£…ï¼ˆè·³è¿‡è™šæ‹Ÿç¯å¢ƒæ£€æŸ¥ï¼Œä¸æ¨èï¼‰ |
| **å¼€å‘ä¸è®­ç»ƒ** | |
| `make test` | è¿è¡Œæ¨¡å‹æµ‹è¯• |
| `make train` | å¼€å§‹è®­ç»ƒæ¨¡å‹ |
| `make train-multimodal` | å¼€å§‹å®Œæ•´å¤šæ¨¡æ€è®­ç»ƒï¼ˆæ–‡æœ¬+å›¾åƒ+è¯­éŸ³ï¼‰ |
| `make serve` | å¯åŠ¨æ¨ç†APIæœåŠ¡ï¼ˆç”Ÿäº§æ¨¡å¼ï¼‰ |
| `make serve-dev` | å¯åŠ¨æ¨ç†APIæœåŠ¡ï¼ˆå¼€å‘æ¨¡å¼ï¼‰ |
| `make generate` | è¿è¡Œæ–‡æœ¬ç”Ÿæˆ |
| `make quick-test` | å¿«é€Ÿæµ‹è¯•ï¼ˆéªŒè¯æ¨¡å‹å¯ç”¨ï¼‰ |

## ğŸ­ å·¥ä¸šåŒ–æ¥å£

æœåŠ¡å¯åŠ¨åæä¾›æ ‡å‡†å¥åº·æ£€æŸ¥ä¸æ¨ç†ç«¯ç‚¹ï¼š

- `GET /healthz`ï¼šè¿›ç¨‹å¥åº·æ£€æŸ¥
- `GET /readyz`ï¼šæ¨¡å‹å°±ç»ªæ£€æŸ¥
- `GET /metrics`ï¼šPrometheusç›‘æ§æŒ‡æ ‡
- `POST /v1/generate`ï¼šæ–‡æœ¬ç”Ÿæˆæ¥å£

### å®‰å…¨ä¸é™æµï¼ˆç”Ÿäº§å»ºè®®ï¼‰

- `LLM_API_KEYS`ï¼šé€—å·åˆ†éš”çš„API Keyåˆ—è¡¨ï¼Œè®¾ç½®å `/v1/generate` å¿…é¡»æºå¸¦è¯·æ±‚å¤´ `X-API-Key`
- `LLM_USERS`ï¼šOAuth2è´¦å·å¯†ç ï¼Œæ ¼å¼ `user1:pass1,user2:pass2`
- `LLM_JWT_SECRET`ï¼šJWTç­¾åå¯†é’¥ï¼ˆç”Ÿäº§å¿…é¡»ä¿®æ”¹ï¼‰
- `LLM_JWT_EXPIRE_MINUTES`ï¼šJWTæœ‰æ•ˆæœŸï¼ˆåˆ†é’Ÿï¼‰
- `LLM_RATE_LIMIT_RPM`ï¼šæ¯åˆ†é’Ÿæ¯ä¸ªè°ƒç”¨æ–¹é™æµï¼ˆ0è¡¨ç¤ºå…³é—­ï¼‰
- `LLM_LOG_LEVEL`ï¼šæ—¥å¿—çº§åˆ«ï¼ˆé»˜è®¤ `INFO`ï¼‰
- `LLM_SESSION_DB`ï¼šä¼šè¯SQLiteæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ `sessions.db`ï¼‰

ç¤ºä¾‹ï¼š

```bash
export LLM_API_KEYS="prod-key-1,prod-key-2"
export LLM_USERS="admin:admin123"
export LLM_JWT_SECRET="replace-with-strong-secret"
export LLM_RATE_LIMIT_RPM=60
make serve
```

OAuth2 è·å– tokenï¼š

```bash
curl -X POST http://127.0.0.1:8000/oauth/token \
    -H "Content-Type: application/x-www-form-urlencoded" \
    -d "username=admin&password=admin123"
```

ä½¿ç”¨ Bearer Token è¯·æ±‚ï¼š

```bash
curl -X POST http://127.0.0.1:8000/v1/generate \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer <access_token>" \
    -d '{"prompt":"Hello","max_new_tokens":64,"session_id":"demo-session"}'
```

ä¼šè¯æ¥å£ï¼š

- `GET /v1/sessions/{session_id}`ï¼šè¯»å–å†å²ä¼šè¯
- `DELETE /v1/sessions/{session_id}`ï¼šåˆ é™¤å†å²ä¼šè¯

è¯·æ±‚ç¤ºä¾‹ï¼ˆå¸¦é‰´æƒï¼‰ï¼š

```bash
curl -X POST http://127.0.0.1:8000/v1/generate \
    -H "Content-Type: application/json" \
    -H "X-API-Key: prod-key-1" \
    -d '{"prompt":"Hello","max_new_tokens":64}'
```

ç¤ºä¾‹è¯·æ±‚ï¼š

```bash
curl -X POST http://127.0.0.1:8000/v1/generate \
    -H "Content-Type: application/json" \
    -d '{"prompt":"Hello","max_new_tokens":64}'
```

### å®¹å™¨åŒ–éƒ¨ç½²

```bash
docker build -t my-llm:latest .
docker run --rm -p 8000:8000 -e LLM_CHECKPOINT=checkpoints/best_model.pt my-llm:latest
```

### Prometheus + Grafana

```bash
make obs-up
```

- Prometheus: http://127.0.0.1:9090
- Grafana: http://127.0.0.1:3000 ï¼ˆé»˜è®¤ admin/adminï¼‰

åœæ­¢ï¼š

```bash
make obs-down
```

### CI/CDï¼ˆé•œåƒæ„å»ºä¸è‡ªåŠ¨å‘å¸ƒï¼‰

å·²æä¾› GitHub Actions å·¥ä½œæµï¼š[.github/workflows/cicd.yml](.github/workflows/cicd.yml)

- PR/Push è‡ªåŠ¨æ‰§è¡Œæµ‹è¯•
- Push åˆ°ä¸»åˆ†æ”¯æˆ– tag è‡ªåŠ¨æ„å»ºå¹¶æ¨é€é•œåƒåˆ° GHCR
| **å·¥å…·** | |
| `make info` | æŸ¥çœ‹æ¨¡å‹é…ç½®ä¿¡æ¯ |
| `make check-deps` | æ£€æŸ¥ä¾èµ–å®‰è£…æƒ…å†µ |
| `make init` | åˆ›å»ºå¿…è¦çš„é¡¹ç›®ç›®å½• |
| **æ¸…ç†** | |
| `make clean` | æ¸…ç†Pythonç¼“å­˜æ–‡ä»¶ |
| `make clean-checkpoints` | åˆ é™¤æ‰€æœ‰checkpoint |
| `make clean-all` | æ¸…ç†æ‰€æœ‰ç”Ÿæˆæ–‡ä»¶ |

## âš™ï¸ é…ç½®è¯´æ˜

### æ¨¡å‹é…ç½® (`ModelConfig`)

```python
vocab_size = 50257    # è¯è¡¨å¤§å°
n_layer = 6           # Transformerå±‚æ•°
n_head = 6            # æ³¨æ„åŠ›å¤´æ•°
n_embd = 384          # åµŒå…¥ç»´åº¦
block_size = 512      # æœ€å¤§åºåˆ—é•¿åº¦
```

### è®­ç»ƒé…ç½® (`TrainConfig`)

```python
batch_size = 16       # æ‰¹æ¬¡å¤§å°
learning_rate = 3e-4  # å­¦ä¹ ç‡
max_iters = 10000     # æœ€å¤§è®­ç»ƒæ­¥æ•°
eval_interval = 500   # è¯„ä¼°é—´éš”
```

## ğŸ“Š æ¨¡å‹æ¶æ„

æœ¬é¡¹ç›®å®ç°äº†åŸºäºGPTçš„è‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼š

1. **Token Embedding + Position Embedding**
2. **å¤šå±‚Transformer Block**
   - å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ (Multi-Head Self-Attention)
   - å‰é¦ˆç¥ç»ç½‘ç»œ (Feed-Forward Network)
   - å±‚å½’ä¸€åŒ– (Layer Normalization)
   - æ®‹å·®è¿æ¥ (Residual Connections)
3. **è¾“å‡ºå±‚** (Language Modeling Head)

## ğŸ“ å­¦ä¹ è·¯å¾„

1. **æ¨¡å‹ç†è§£**: ä» [`model.py`](model.py) å¼€å§‹ï¼Œç†è§£Transformeræ¶æ„
2. **æ•°æ®å¤„ç†**: æŸ¥çœ‹ [`data.py`](data.py) äº†è§£æ•°æ®å‡†å¤‡æµç¨‹
3. **è®­ç»ƒè¿‡ç¨‹**: é˜…è¯» [`train.py`](train.py) å­¦ä¹ è®­ç»ƒå¾ªç¯
4. **æ–‡æœ¬ç”Ÿæˆ**: æ¢ç´¢ [`generate.py`](generate.py) äº†è§£æ¨ç†è¿‡ç¨‹

### ğŸ“– å®Œæ•´æ–‡æ¡£ç³»ç»Ÿ

**ğŸ‘‹ é¦–æ¬¡ä½¿ç”¨ï¼Ÿè¯·ä»è¿™é‡Œå¼€å§‹ï¼š** [**docs/START_HERE.md**](docs/START_HERE.md)

æ‰€æœ‰é¡¹ç›®æ–‡æ¡£éƒ½åœ¨ **docs/** ç›®å½•ã€‚ä»¥ä¸‹æ˜¯ä¸»è¦æ–‡æ¡£ï¼š

**å¿«é€Ÿå‚è€ƒ**:
- ğŸš€ [START_HERE.md](docs/START_HERE.md) - æ–°ç”¨æˆ·å…¥é—¨æŒ‡å—
- âš¡ [QUICK_REFERENCE.md](docs/QUICK_REFERENCE.md) - 1é¡µé€ŸæŸ¥å¡
- ğŸ“‹ [CHEATSHEET.md](docs/CHEATSHEET.md) - æ ¸å¿ƒå‘½ä»¤é€Ÿè®°
- ğŸ“– [commands_reference.md](docs/commands_reference.md) - å®Œæ•´å‘½ä»¤å‚è€ƒ

**æ·±åº¦å­¦ä¹ **:
- ğŸ” [checkpoint_system.md](docs/checkpoint_system.md) - æ£€æŸ¥ç‚¹ç³»ç»Ÿè¯¦è§£
- ğŸ“Š [training_visualization.md](docs/training_visualization.md) - å®æ—¶è®­ç»ƒç›‘æ§
- ğŸ­ [openai_training_guide.md](docs/openai_training_guide.md) - å·¥ä¸šçº§è®­ç»ƒæŒ‡å—
- âš–ï¸ [openai_vs_local_comparison.md](docs/openai_vs_local_comparison.md) - OpenAIå¯¹æ¯”åˆ†æ

**æ›´æ–°æ–‡æ¡£**:
- ğŸ“ [UPDATE_SUMMARY.md](docs/UPDATE_SUMMARY.md) - å®Œæ•´æ›´æ–°è¯´æ˜
- ğŸ¯ [CHECKPOINT_UPDATE.md](docs/CHECKPOINT_UPDATE.md) - æ£€æŸ¥ç‚¹æ”¹è¿›è¯´æ˜
- ğŸ“Š [BEFORE_AND_AFTER.md](docs/BEFORE_AND_AFTER.md) - æ”¹è¿›å‰åå¯¹æ¯”

**æ–‡æ¡£å¯¼èˆª**:
- ğŸ—ºï¸ [README_DOCS.md](docs/README_DOCS.md) - æ–‡æ¡£å¯¼èˆªå’Œç´¢å¼•
- ğŸ“š [TRAINING_README.md](docs/TRAINING_README.md) - è®­ç»ƒç³»ç»Ÿå®Œæ•´è¯´æ˜

## ğŸ“ˆ æ‰©å±•å»ºè®®

### å¢åŠ æ¨¡å‹è§„æ¨¡

ä¿®æ”¹ `config.py` ä¸­çš„å‚æ•°ï¼š

```python
# å°æ¨¡å‹ï¼ˆå½“å‰ï¼‰
n_layer = 6
n_embd = 384
# çº¦ 30M å‚æ•°

# ä¸­ç­‰æ¨¡å‹
n_layer = 12
n_embd = 768
# çº¦ 117M å‚æ•°

# å¤§æ¨¡å‹
n_layer = 24
n_embd = 1024
# çº¦ 345M å‚æ•°
```

### ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®

ä¿®æ”¹ [`data.py`](data.py) ä¸­çš„æ•°æ®åŠ è½½å‡½æ•°ï¼Œæˆ–å‡†å¤‡è‡ªå·±çš„æ–‡æœ¬æ•°æ®ï¼š

```python
# ä½¿ç”¨æœ¬åœ°æ–‡æœ¬æ–‡ä»¶
with open('my_data.txt', 'r') as f:
    text = f.read()
# ç„¶åè¿›è¡Œåˆ†è¯å’Œå¤„ç†
```

### æ·»åŠ é«˜çº§åŠŸèƒ½

- **æ··åˆç²¾åº¦è®­ç»ƒ**: å·²æ”¯æŒï¼ŒåŠ é€Ÿè®­ç»ƒ
- **æ¢¯åº¦ç´¯ç§¯**: æ¨¡æ‹Ÿæ›´å¤§çš„batch size
- **åˆ†å¸ƒå¼è®­ç»ƒ**: ä½¿ç”¨DDPè¿›è¡Œå¤šGPUè®­ç»ƒ
- **æ›´å¥½çš„é‡‡æ ·ç­–ç•¥**: Top-p (nucleus) sampling
- **Checkpointå¹³å‡**: æå‡æ¨¡å‹ç¨³å®šæ€§

## ğŸ”§ å¸¸è§é—®é¢˜

### Q: å®‰è£…ä¾èµ–æ—¶å‡ºç° "externally-managed-environment" é”™è¯¯ï¼Ÿ
A: 
è¿™æ˜¯æ–°ç‰ˆ Linux ç³»ç»Ÿçš„å®‰å…¨ç‰¹æ€§ï¼Œéœ€è¦ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒï¼š
```bash
# ä¸€é”®è§£å†³
make setup-all

# ç„¶åæ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source venv/bin/activate

# æˆ–è€…åˆ†æ­¥æ‰§è¡Œ
make setup              # åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
source venv/bin/activate  # æ¿€æ´»
make install            # å®‰è£…ä¾èµ–
```

### Q: è®­ç»ƒå¾ˆæ…¢æ€ä¹ˆåŠï¼Ÿ
A: 
- å‡å°æ¨¡å‹è§„æ¨¡æˆ–batch_size
- ä½¿ç”¨GPUï¼ˆCUDAï¼‰è€Œä¸æ˜¯CPU
- å¯ç”¨ `torch.compile`ï¼ˆPyTorch 2.0+ï¼‰

### Q: å†…å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ
A:
- å‡å° `batch_size`
- å‡å° `block_size`
- å‡å°æ¨¡å‹å‚æ•°ï¼ˆn_layer, n_embdï¼‰

### Q: å¦‚ä½•ä½¿ç”¨æ›´å¤§çš„æ•°æ®é›†ï¼Ÿ
A:
- ä¿®æ”¹ `data.py` ä¸­çš„ `dataset_name` å’Œ `dataset_config`
- æˆ–å®ç°è‡ªå®šä¹‰æ•°æ®åŠ è½½å™¨

## ğŸ“š å‚è€ƒèµ„æ–™

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - TransformeråŸè®ºæ–‡
- [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) - GPT-2è®ºæ–‡
- [nanoGPT](https://github.com/karpathy/nanoGPT) - Andrej Karpathyçš„GPTå®ç°
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) - Transformerå¯è§†åŒ–è®²è§£

## ğŸ“ License

MIT License