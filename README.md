# ä»é›¶å¼€å‘å¤§è¯­è¨€æ¨¡å‹ (LLM)

ä¸€ä¸ªä»é›¶å®ç°çš„GPTé£æ ¼å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒé¡¹ç›®ï¼ŒåŒ…å«å®Œæ•´çš„è®­ç»ƒã€è¯„ä¼°å’Œç”ŸæˆåŠŸèƒ½ã€‚

## ğŸŒŸ ç‰¹æ€§

- ğŸ”§ **å®Œæ•´å®ç°**: ä»æ•°æ®å¤„ç†åˆ°æ¨¡å‹è®­ç»ƒçš„å®Œæ•´æµç¨‹
- ğŸ¯ **GPTæ¶æ„**: åŸºäºTransformerçš„è‡ªå›å½’è¯­è¨€æ¨¡å‹
- ğŸ“Š **å¯é…ç½®**: çµæ´»çš„æ¨¡å‹å’Œè®­ç»ƒé…ç½®
- ğŸš€ **æ˜“ç”¨**: ç®€æ´çš„APIå’Œæ¸…æ™°çš„ä»£ç ç»“æ„
- ğŸ’¡ **æ•™è‚²æ€§**: æ³¨é‡Šè¯¦å°½ï¼Œé€‚åˆå­¦ä¹ 

## ğŸ“ é¡¹ç›®ç»“æ„

```
llm/
â”œâ”€â”€ config.py          # æ¨¡å‹å’Œè®­ç»ƒé…ç½®
â”œâ”€â”€ model.py           # GPTæ¨¡å‹å®ç°
â”œâ”€â”€ data.py            # æ•°æ®åŠ è½½å’Œå¤„ç†
â”œâ”€â”€ train.py           # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ generate.py        # æ–‡æœ¬ç”Ÿæˆè„šæœ¬
â”œâ”€â”€ requirements.txt   # ä¾èµ–åŒ…
â””â”€â”€ checkpoints/       # æ¨¡å‹ä¿å­˜ç›®å½•ï¼ˆè‡ªåŠ¨åˆ›å»ºï¼‰
```

## ğŸš€ å¿«é€Ÿå¼€å§‹
> ğŸ’¡ **é‡åˆ°å®‰è£…é—®é¢˜ï¼Ÿ** æŸ¥çœ‹è¯¦ç»†çš„ [å®‰è£…æŒ‡å— (INSTALL.md)](INSTALL.md)


### æ–¹å¼ä¸€ï¼šä½¿ç”¨ Makefileï¼ˆæ¨èï¼‰

```bash
# ä¸€é”®è®¾ç½®ï¼ˆåˆ›å»ºè™šæ‹Ÿç¯å¢ƒå¹¶å®‰è£…æ‰€æœ‰ä¾èµ–ï¼‰
make setup-all

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source venv/bin/activate  # Linux/Mac
# æˆ– venv\Scripts\activate  # Windows

# æµ‹è¯•æ¨¡å‹
make test

# è®­ç»ƒæ¨¡å‹
make train

# ç”Ÿæˆæ–‡æœ¬
make generate
```

**æˆ–è€…åˆ†æ­¥æ‰§è¡Œï¼š**

```bash
# 1. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
make setup

# 2. æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source venv/bin/activate  # Linux/Mac

# 3. å®‰è£…ä¾èµ–
make install

# 4. æŸ¥çœ‹æ‰€æœ‰å¯ç”¨å‘½ä»¤
make help
```

### æ–¹å¼äºŒï¼šç›´æ¥è¿è¡ŒPythonè„šæœ¬

#### 1. å®‰è£…ä¾èµ–

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰
python -m venv venv
source venv/bin/activate  # Linux/Mac
# æˆ– venv\Scripts\activate  # Windows

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

#### 2. è®­ç»ƒæ¨¡å‹

```bash
python train.py
```

è®­ç»ƒå°†ä½¿ç”¨WikiText-2æ•°æ®é›†ï¼Œæ¨¡å‹checkpointä¼šä¿å­˜åœ¨ `checkpoints/` ç›®å½•ã€‚

#### 3. ç”Ÿæˆæ–‡æœ¬

è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿›è¡Œæ–‡æœ¬ç”Ÿæˆï¼š

```bash
python generate.py
```

## ğŸ”§ Makefile å‘½ä»¤å‚è€ƒ

| å‘½ä»¤ | è¯´æ˜ |
|------|------|
| `make help` | æ˜¾ç¤ºæ‰€æœ‰å¯ç”¨å‘½ä»¤ |
| **ç¯å¢ƒè®¾ç½®** | |
| `make setup-all` | â­ ä¸€é”®è®¾ç½®ï¼ˆåˆ›å»ºè™šæ‹Ÿç¯å¢ƒ+å®‰è£…ä¾èµ–ï¼‰ |
| `make setup` | ä»…åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ |
| `make install` | å®‰è£…ä¾èµ–ï¼ˆéœ€è¦å…ˆæ¿€æ´»è™šæ‹Ÿç¯å¢ƒï¼‰ |
| `make install-force` | å¼ºåˆ¶å®‰è£…ï¼ˆè·³è¿‡è™šæ‹Ÿç¯å¢ƒæ£€æŸ¥ï¼Œä¸æ¨èï¼‰ |
| **å¼€å‘ä¸è®­ç»ƒ** | |
| `make test` | è¿è¡Œæ¨¡å‹æµ‹è¯• |
| `make train` | å¼€å§‹è®­ç»ƒæ¨¡å‹ |
| `make generate` | è¿è¡Œæ–‡æœ¬ç”Ÿæˆ |
| `make quick-test` | å¿«é€Ÿæµ‹è¯•ï¼ˆéªŒè¯æ¨¡å‹å¯ç”¨ï¼‰ |
| **å·¥å…·** | |
| `make info` | æŸ¥çœ‹æ¨¡å‹é…ç½®ä¿¡æ¯ |
| `make check-deps` | æ£€æŸ¥ä¾èµ–å®‰è£…æƒ…å†µ |
| `make init` | åˆ›å»ºå¿…è¦çš„é¡¹ç›®ç›®å½• |
| **æ¸…ç†** | |
| `make clean` | æ¸…ç†Pythonç¼“å­˜æ–‡ä»¶ |
| `make clean-checkpoints` | åˆ é™¤æ‰€æœ‰checkpoint |
| `make clean-all` | æ¸…ç†æ‰€æœ‰ç”Ÿæˆæ–‡ä»¶ |

## âš™ï¸ é…ç½®è¯´æ˜

### æ¨¡å‹é…ç½® (`ModelConfig`)

```python
vocab_size = 50257    # è¯è¡¨å¤§å°
n_layer = 6           # Transformerå±‚æ•°
n_head = 6            # æ³¨æ„åŠ›å¤´æ•°
n_embd = 384          # åµŒå…¥ç»´åº¦
block_size = 512      # æœ€å¤§åºåˆ—é•¿åº¦
```

### è®­ç»ƒé…ç½® (`TrainConfig`)

```python
batch_size = 16       # æ‰¹æ¬¡å¤§å°
learning_rate = 3e-4  # å­¦ä¹ ç‡
max_iters = 10000     # æœ€å¤§è®­ç»ƒæ­¥æ•°
eval_interval = 500   # è¯„ä¼°é—´éš”
```

## ğŸ“Š æ¨¡å‹æ¶æ„

æœ¬é¡¹ç›®å®ç°äº†åŸºäºGPTçš„è‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼š

1. **Token Embedding + Position Embedding**
2. **å¤šå±‚Transformer Block**
   - å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ (Multi-Head Self-Attention)
   - å‰é¦ˆç¥ç»ç½‘ç»œ (Feed-Forward Network)
   - å±‚å½’ä¸€åŒ– (Layer Normalization)
   - æ®‹å·®è¿æ¥ (Residual Connections)
3. **è¾“å‡ºå±‚** (Language Modeling Head)

## ğŸ“ å­¦ä¹ è·¯å¾„

1. **æ¨¡å‹ç†è§£**: ä» [`model.py`](model.py) å¼€å§‹ï¼Œç†è§£Transformeræ¶æ„
2. **æ•°æ®å¤„ç†**: æŸ¥çœ‹ [`data.py`](data.py) äº†è§£æ•°æ®å‡†å¤‡æµç¨‹
3. **è®­ç»ƒè¿‡ç¨‹**: é˜…è¯» [`train.py`](train.py) å­¦ä¹ è®­ç»ƒå¾ªç¯
4. **æ–‡æœ¬ç”Ÿæˆ**: æ¢ç´¢ [`generate.py`](generate.py) äº†è§£æ¨ç†è¿‡ç¨‹

## ğŸ“ˆ æ‰©å±•å»ºè®®

### å¢åŠ æ¨¡å‹è§„æ¨¡

ä¿®æ”¹ `config.py` ä¸­çš„å‚æ•°ï¼š

```python
# å°æ¨¡å‹ï¼ˆå½“å‰ï¼‰
n_layer = 6
n_embd = 384
# çº¦ 30M å‚æ•°

# ä¸­ç­‰æ¨¡å‹
n_layer = 12
n_embd = 768
# çº¦ 117M å‚æ•°

# å¤§æ¨¡å‹
n_layer = 24
n_embd = 1024
# çº¦ 345M å‚æ•°
```

### ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®

ä¿®æ”¹ [`data.py`](data.py) ä¸­çš„æ•°æ®åŠ è½½å‡½æ•°ï¼Œæˆ–å‡†å¤‡è‡ªå·±çš„æ–‡æœ¬æ•°æ®ï¼š

```python
# ä½¿ç”¨æœ¬åœ°æ–‡æœ¬æ–‡ä»¶
with open('my_data.txt', 'r') as f:
    text = f.read()
# ç„¶åè¿›è¡Œåˆ†è¯å’Œå¤„ç†
```

### æ·»åŠ é«˜çº§åŠŸèƒ½

- **æ··åˆç²¾åº¦è®­ç»ƒ**: å·²æ”¯æŒï¼ŒåŠ é€Ÿè®­ç»ƒ
- **æ¢¯åº¦ç´¯ç§¯**: æ¨¡æ‹Ÿæ›´å¤§çš„batch size
- **åˆ†å¸ƒå¼è®­ç»ƒ**: ä½¿ç”¨DDPè¿›è¡Œå¤šGPUè®­ç»ƒ
- **æ›´å¥½çš„é‡‡æ ·ç­–ç•¥**: Top-p (nucleus) sampling
- **Checkpointå¹³å‡**: æå‡æ¨¡å‹ç¨³å®šæ€§

## ğŸ”§ å¸¸è§é—®é¢˜

### Q: å®‰è£…ä¾èµ–æ—¶å‡ºç° "externally-managed-environment" é”™è¯¯ï¼Ÿ
A: 
è¿™æ˜¯æ–°ç‰ˆ Linux ç³»ç»Ÿçš„å®‰å…¨ç‰¹æ€§ï¼Œéœ€è¦ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒï¼š
```bash
# ä¸€é”®è§£å†³
make setup-all

# ç„¶åæ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source venv/bin/activate

# æˆ–è€…åˆ†æ­¥æ‰§è¡Œ
make setup              # åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
source venv/bin/activate  # æ¿€æ´»
make install            # å®‰è£…ä¾èµ–
```

### Q: è®­ç»ƒå¾ˆæ…¢æ€ä¹ˆåŠï¼Ÿ
A: 
- å‡å°æ¨¡å‹è§„æ¨¡æˆ–batch_size
- ä½¿ç”¨GPUï¼ˆCUDAï¼‰è€Œä¸æ˜¯CPU
- å¯ç”¨ `torch.compile`ï¼ˆPyTorch 2.0+ï¼‰

### Q: å†…å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ
A:
- å‡å° `batch_size`
- å‡å° `block_size`
- å‡å°æ¨¡å‹å‚æ•°ï¼ˆn_layer, n_embdï¼‰

### Q: å¦‚ä½•ä½¿ç”¨æ›´å¤§çš„æ•°æ®é›†ï¼Ÿ
A:
- ä¿®æ”¹ `data.py` ä¸­çš„ `dataset_name` å’Œ `dataset_config`
- æˆ–å®ç°è‡ªå®šä¹‰æ•°æ®åŠ è½½å™¨

## ğŸ“š å‚è€ƒèµ„æ–™

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - TransformeråŸè®ºæ–‡
- [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) - GPT-2è®ºæ–‡
- [nanoGPT](https://github.com/karpathy/nanoGPT) - Andrej Karpathyçš„GPTå®ç°
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) - Transformerå¯è§†åŒ–è®²è§£

## ğŸ“ License

MIT License

## ğŸ™ è‡´è°¢

æœ¬é¡¹ç›®å— [nanoGPT](https://github.com/karpathy/nanoGPT) å¯å‘ï¼Œæ—¨åœ¨æä¾›ä¸€ä¸ªæ¸…æ™°æ˜“æ‡‚çš„LLMå®ç°ã€‚