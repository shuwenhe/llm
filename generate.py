"""æ–‡æœ¬ç”Ÿæˆè„šæœ¬"""
import torch
from model import GPT
from config import ModelConfig
from data import load_tokenizer


def load_model(checkpoint_path):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    # åˆ›å»ºæ¨¡å‹é…ç½®
    model_config = ModelConfig(**checkpoint['model_config'])
    
    # åˆ›å»ºæ¨¡å‹å¹¶åŠ è½½æƒé‡
    model = GPT(model_config)
    model.load_state_dict(checkpoint['model'])
    
    return model, model_config


def generate_text(prompt, model, tokenizer, device, max_new_tokens=100, temperature=0.8, top_k=40):
    """ç”Ÿæˆæ–‡æœ¬"""
    model.eval()
    model.to(device)
    
    # ç¼–ç è¾“å…¥
    tokens = tokenizer.encode(prompt, return_tensors='pt').to(device)
    
    print(f"\næç¤ºè¯: {prompt}")
    print(f"ç”Ÿæˆä¸­...\n")
    
    # ç”Ÿæˆ
    with torch.no_grad():
        generated_tokens = model.generate(
            tokens,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_k=top_k
        )
    
    # è§£ç 
    generated_text = tokenizer.decode(generated_tokens[0].tolist())
    return generated_text


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®
    checkpoint_path = "checkpoints/best_model.pt"
    device = torch.device("cuda" if torch.cuda.is_available() else 
                         "mps" if torch.backends.mps.is_available() else "cpu")
    
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"åŠ è½½æ¨¡å‹: {checkpoint_path}")
    
    # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    model, config = load_model(checkpoint_path)
    tokenizer = load_tokenizer()
    
    print(f"æ¨¡å‹å‚æ•°é‡: {model.get_num_params()/1e6:.2f}M")
    
    # é¢„è®¾å‚æ•°
    presets = {
        '1': {'name': 'ä¿å®ˆæ¨¡å¼', 'temp': 0.7, 'top_k': 50, 'tokens': 150},
        '2': {'name': 'å¹³è¡¡æ¨¡å¼', 'temp': 0.8, 'top_k': 200, 'tokens': 250},
        '3': {'name': 'åˆ›æ„æ¨¡å¼', 'temp': 1.0, 'top_k': 300, 'tokens': 300},
    }
    
    # äº¤äº’å¼ç”Ÿæˆ
    print("\n" + "="*50)
    print("æ–‡æœ¬ç”Ÿæˆå™¨ (è¾“å…¥ 'quit' é€€å‡º)")
    print("="*50)
    print("\nğŸ“ ç”Ÿæˆæ¨¡å¼:")
    for k, v in presets.items():
        print(f"  {k}. {v['name']} (temperature={v['temp']}, top_k={v['top_k']}, tokens={v['tokens']})")
    print("\nğŸ’¡ æç¤ºè¯ç¤ºä¾‹:")
    print("  - Once upon a time")
    print("  - The meaning of life is")
    print("  - In a world where")
    
    # é»˜è®¤ä½¿ç”¨å¹³è¡¡æ¨¡å¼
    current_preset = presets['2']
    
    while True:
        prompt = input("\nè¯·è¾“å…¥æç¤ºè¯ (æˆ–è¾“å…¥1/2/3åˆ‡æ¢æ¨¡å¼): ")
        
        if prompt.lower() == 'quit':
            break
        
        # åˆ‡æ¢æ¨¡å¼
        if prompt in presets:
            current_preset = presets[prompt]
            print(f"âœ“ å·²åˆ‡æ¢åˆ°: {current_preset['name']}")
            continue
        
        if not prompt.strip():
            continue
        
        # ç”Ÿæˆæ–‡æœ¬
        generated = generate_text(
            prompt,
            model,
            tokenizer,
            device,
            max_new_tokens=current_preset['tokens'],
            temperature=current_preset['temp'],
            top_k=current_preset['top_k']
        )
        
        print(f"\nç”Ÿæˆç»“æœ [{current_preset['name']}]:\n{generated}\n")
        print("-"*50)


if __name__ == "__main__":
    main()
